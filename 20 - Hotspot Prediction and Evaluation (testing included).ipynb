{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date, datetime, timedelta\n",
    "import pickle\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Hotspot calculation (Also used in GUI) <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(file_name):\n",
    "    try:\n",
    "        data_file = pd.read_csv(file_name + CSV_FILE_EXTENSION)\n",
    "    except:\n",
    "        print(\"File not found\")\n",
    "        return pd.DataFrame()\n",
    "    return data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file variables\n",
    "GUI_FILE_NAME = \"CrimeGUI/data\"\n",
    "CSV_FILE_EXTENSION = \".csv\"\n",
    "df = pd.DataFrame()\n",
    "FILTERED_DATA = pd.DataFrame()\n",
    "MODEL_FEATURE_SEPARATOR = \"_\"\n",
    "MODEL_PATH = \"CrimeGUI/Models/\"\n",
    "COMMA_SPACE = \", \"\n",
    "INCIDENT_COL_KEY = \"Todays Reports\"\n",
    "NEIGHBOURHOOD_COL_KEY = \"Neighborhood\"\n",
    "DATE_COL_KEY = \"Date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature selection variables\n",
    "file = open(\"Selection Methods\",\"rb\")\n",
    "sel_methods = np.load(file)\n",
    "F_REGRESSION_NAME = \"F-Regression\"\n",
    "F_REGRESSION_FILE_TAG = \"f_regression\"\n",
    "CHI2_NAME = \"Chi-Squared\"\n",
    "CHI2_FILE_TAG = \"chi2\"\n",
    "ADABOOST_NAME = \"AdaBoost\"\n",
    "ADABOOST_FILE_TAG = \"adaboost\"\n",
    "EQUAL_DATA_NAME = \"Equal Selection\"\n",
    "EQUAL_DATA_FILE_TAG = \"equal_crime_and_business\"\n",
    "ALL_BUS_NAME = \"All Business\"\n",
    "ALL_BUS_FILE_TAG = \"all_business\"\n",
    "FEATURE_SELECTION = [F_REGRESSION_NAME, CHI2_NAME, ADABOOST_NAME, EQUAL_DATA_NAME, ALL_BUS_NAME]\n",
    "FEATURES = {\n",
    "    F_REGRESSION_NAME : ['Reports 1 day ago', 'Reports 2 days ago', 'Reports 3 days ago',\n",
    "                       'Reports 4 days ago', 'Reports 5 days ago', 'Reports 6 days ago',\n",
    "                      'Reports 7 days ago','Reports 14 days ago','Reports 30 days ago','Reports 365 days ago'],\n",
    "    CHI2_NAME : ['South of Market', 'Mission', 'Tenderloin', 'Number of businesses', \n",
    "               'Downtown / Union Square', 'Civic Center', 'Reports 365 days ago',\n",
    "               'Reports 1 day ago','Reports 2 days ago','Reports 14 days ago'],\n",
    "    ADABOOST_NAME : ['Reports 365 days ago', 'Reports 1 day ago', 'Reports 14 days ago', 'Reports 3 days ago', \n",
    "               'Reports 2 days ago', 'Reports 7 days ago', 'Number of businesses',\n",
    "               'Reports 4 days ago','Reports 5 days ago','Closures 365 days ago'],\n",
    "    EQUAL_DATA_NAME : ['Number of businesses', 'Last 28 days closures', 'Last 7 days openings',\n",
    "                          'Last 14 days closures', 'Last 7 days closures','Reports 1 day ago',\n",
    "                      'Reports 2 days ago', 'Reports 4 days ago', 'Reports 30 days ago', 'Reports 7 days ago'],\n",
    "    ALL_BUS_NAME : ['Number of businesses', 'Last 28 days closures', 'Last 7 days openings',\n",
    "                          'Last 14 days closures', 'Last 7 days closures','Number of openings',\n",
    "                   'Openings 4 days ago','Openings 1 day ago', 'Openings 7 days ago', 'Openings 2 days ago']\n",
    "    }\n",
    "FEATURE_FILE_TAGS = {\n",
    "    F_REGRESSION_NAME : F_REGRESSION_FILE_TAG,\n",
    "    CHI2_NAME : CHI2_FILE_TAG,\n",
    "    ADABOOST_NAME : ADABOOST_FILE_TAG,\n",
    "    EQUAL_DATA_NAME : EQUAL_DATA_FILE_TAG,\n",
    "    ALL_BUS_NAME : ALL_BUS_FILE_TAG\n",
    "    }\n",
    "FEATURE_NAMES_BY_FILE_TAG = {\n",
    "    F_REGRESSION_FILE_TAG : F_REGRESSION_NAME,\n",
    "    CHI2_FILE_TAG : CHI2_NAME,\n",
    "    ADABOOST_FILE_TAG : ADABOOST_NAME,\n",
    "    EQUAL_DATA_FILE_TAG : EQUAL_DATA_NAME,\n",
    "    ALL_BUS_FILE_TAG : ALL_BUS_NAME\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model variables\n",
    "ANN_NAME = \"Multi-Layer\\nPerceptron\"\n",
    "ANN_FILE_TAG = \"multi_layer_perceptron\"\n",
    "DECISION_TREE_NAME = \"Decision Tree\"\n",
    "DECISION_TREE_FILE_TAG = \"decision_tree\"\n",
    "ELASTIC_NET_NAME = \"Elastic Net\"\n",
    "ELASTIC_NET_FILE_TAG = \"elastic_net\"\n",
    "LASSO_NAME = \"Lasso\"\n",
    "LASSO_FILE_TAG = \"lasso\"\n",
    "LINERAR_REGRESSION_NAME = \"Linear \\nRegression\"\n",
    "LINERAR_REGRESSION_FILE_TAG = \"linear_regression\"\n",
    "RANDOM_FOREST_NAME = \"Random \\nForest\"\n",
    "RANDOM_FOREST_FILE_TAG = \"random_forest\"\n",
    "RIDGE_REGRESSION_NAME = \"Ridge \\nRegression\"\n",
    "RIDGE_REGRESSION_FILE_TAG = \"ridge_regression\"\n",
    "SVM_NAME = \"SVM\"\n",
    "SVM_FILE_TAG = \"svm\"\n",
    "MODELS = [ANN_NAME,\n",
    "          DECISION_TREE_NAME,\n",
    "          ELASTIC_NET_NAME,\n",
    "          LASSO_NAME,\n",
    "          LINERAR_REGRESSION_NAME,\n",
    "          RANDOM_FOREST_NAME,\n",
    "          RIDGE_REGRESSION_NAME,\n",
    "          SVM_NAME]\n",
    "MODEL_FILE_TAGS = {\n",
    "    ANN_NAME : ANN_FILE_TAG,\n",
    "    DECISION_TREE_NAME : DECISION_TREE_FILE_TAG,\n",
    "    ELASTIC_NET_NAME : ELASTIC_NET_FILE_TAG,\n",
    "    LASSO_NAME : LASSO_FILE_TAG,\n",
    "    LINERAR_REGRESSION_NAME: LINERAR_REGRESSION_FILE_TAG,\n",
    "    RANDOM_FOREST_NAME: RANDOM_FOREST_FILE_TAG,\n",
    "    RIDGE_REGRESSION_NAME: RIDGE_REGRESSION_FILE_TAG,\n",
    "    SVM_NAME : SVM_FILE_TAG\n",
    "    }\n",
    "MODEL_NAMES_BY_FILE_TAG = {\n",
    "    ANN_FILE_TAG : ANN_NAME,\n",
    "    DECISION_TREE_FILE_TAG : DECISION_TREE_NAME,\n",
    "    ELASTIC_NET_FILE_TAG : ELASTIC_NET_NAME,\n",
    "    LASSO_FILE_TAG : LASSO_NAME,\n",
    "    LINERAR_REGRESSION_FILE_TAG : LINERAR_REGRESSION_NAME,\n",
    "    RANDOM_FOREST_FILE_TAG: RANDOM_FOREST_NAME,\n",
    "    RIDGE_REGRESSION_FILE_TAG : RIDGE_REGRESSION_NAME,\n",
    "    SVM_FILE_TAG : SVM_NAME\n",
    "}\n",
    "algorithms = ['multi_layer_perceptron',\n",
    "                  'decision_tree',\n",
    "                  'elastic_net',\n",
    "                  'lasso',\n",
    "                  'linear_regression',\n",
    "                  'random_forest',\n",
    "                  'ridge_regression',\n",
    "                  'svm']\n",
    "algorithm_display_names = ['Multi\\nLayer\\nPerceptron',\n",
    "                               'Decision\\nTree',\n",
    "                               'Elastic Net',\n",
    "                               'Lasso',\n",
    "                               'Linear\\nRegression',\n",
    "                               'Random\\nForest',\n",
    "                               'Ridge\\nRegression',\n",
    "                               'SVM']\n",
    "feature_select_display_names = ['F Regression',\n",
    "                               'Chi2',\n",
    "                               'Adaboost',\n",
    "                               'Equal crime\\nand business',\n",
    "                               'All Business']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neighbourhoods which contain other neighbourhoods\n",
    "PARENT_NEIGHBOURHOODS = {\n",
    "    \"Central Waterfront\" : [\"Dogpatch\"],\n",
    "    \"Eureka Valley\" : [\"Dolores Heights\",\"Castro\"],\n",
    "    \"Buena Vista\" : [\"Ashbury Heights\"],\n",
    "    \"Cole Valley\" : [\"Parnassus Heights\"],\n",
    "    \"Bayview\" : [\"Apparel City\", \"Produce Market\"],\n",
    "    \"Russian Hill\" : [\"Aquatic Park / Ft. Mason\"],\n",
    "    \"North Beach\" : [\"Bret Harte\"],\n",
    "    \"Western Addition\" : [\"Cathedral Hill\", \"Japantown\"],\n",
    "    \"Downtown / Union Square\" : [\"Fairmount\", \"Chinatown\", \"Lower Nob Hill\", \"Polk Gulch\"],\n",
    "    \"Mission Terrace\" : [\"Cayuga\"],\n",
    "    \"Northern Waterfront\" : [\"Fishermans Wharf\"],\n",
    "    \"Bernal Heights\" : [\"Holly Park\", \"Peralta Heights\", \"St. Marys Park\"],\n",
    "    \"Hunters Point\" : [\"India Basin\"],\n",
    "    \"Forest Hill\" : [\"Laguna Honda\"],\n",
    "    \"Hayes Valley\" : [\"Lower Haight\"],\n",
    "    \"Portola\" : [\"McLaren Park\", \"University Mound\"],\n",
    "    \"South of Market\" : [\"Mint Hill\"],\n",
    "    \"Stonestown\" : [\"Parkmerced\"],\n",
    "    \"Presidio Heights\" : [\"Presidio Terrace\"],\n",
    "    \"South Beach\" : [\"Rincon Hill\"],\n",
    "    \"Potrero Hill\" : [\"Showplace Square\"],\n",
    "    \"Visitacion Valley\" : [\"Sunnydale\"],\n",
    "    \"Lincoln Park / Ft. Miley\" : [\"Sutro Heights\"],\n",
    "    \"Cow Hollow\" : [\"Union Street\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain hotspots based on date and model and feature key\n",
    "def get_hotspots(data, model_key, features_key,calendar_date):\n",
    "    x_data = data.loc[data[DATE_COL_KEY].str.contains(calendar_date)]\n",
    "    neighbourhoods_data = pd.DataFrame(x_data[NEIGHBOURHOOD_COL_KEY])\n",
    "    neighbourhoods_data.reset_index(drop=True, inplace=True)\n",
    "    y_data = pd.DataFrame(x_data[INCIDENT_COL_KEY])\n",
    "    y_data.reset_index(drop=True, inplace=True)\n",
    "    features_selected = FEATURES[features_key]\n",
    "    x_data = x_data[features_selected]\n",
    "    return load_model(x_data, y_data, neighbourhoods_data, model_key, features_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load machine learning model\n",
    "def load_model(x_data, y_data, neighbourhoods_data, model_key, features_key):\n",
    "    model_tag = MODEL_FILE_TAGS[model_key]\n",
    "    feature_tag = FEATURE_FILE_TAGS[features_key]\n",
    "    file_path = MODEL_PATH + model_tag + MODEL_FEATURE_SEPARATOR + feature_tag\n",
    "    with open(file_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "        return make_prediction(model, x_data, y_data, neighbourhoods_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict incident values\n",
    "def make_prediction(model, x_data, y_data, neighbourhoods_data):\n",
    "    y_predict = model.predict(x_data)\n",
    "    y_actual,y_predict,neighbourhoods_data = merge_sub_neighbourhoods(y_data,y_predict,neighbourhoods_data)\n",
    "    total_predictions = len(y_predict)\n",
    "    neighbourhoods_data.reset_index(drop=True, inplace=True)\n",
    "    prediction_neighbourhoods = neighbourhoods_data[NEIGHBOURHOOD_COL_KEY].to_numpy()\n",
    "    actual_neighbourhoods = prediction_neighbourhoods.copy()\n",
    "    indexes = y_actual.argsort()\n",
    "    y_actual = np.flip(y_actual[indexes])\n",
    "    actual_neighbourhoods = np.flip(actual_neighbourhoods[indexes])\n",
    "    indexes = y_predict.argsort()\n",
    "    y_predict = np.flip(y_predict[indexes])\n",
    "    prediction_neighbourhoods = np.flip(prediction_neighbourhoods[indexes])\n",
    "    return y_actual, actual_neighbourhoods, y_predict, prediction_neighbourhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert negative value to zero if necessary\n",
    "def get_non_negative_value(value):\n",
    "    if value < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum incident values for neighbourhoods containing other neighbourhoods\n",
    "def merge_sub_neighbourhoods(y_data,y_predict,neighbourhoods_data):\n",
    "    y_data = pd.DataFrame(y_data).to_numpy().flatten()\n",
    "    indexes_to_remove = []\n",
    "    for parent_key in PARENT_NEIGHBOURHOODS:\n",
    "        parent_index = neighbourhoods_data.index[neighbourhoods_data[NEIGHBOURHOOD_COL_KEY] == parent_key].tolist()[0]\n",
    "        y_predict_parent_value = get_non_negative_value(y_predict[parent_index])\n",
    "        y_data_parent_value = get_non_negative_value(y_data[parent_index])\n",
    "        for sub_neighbourhood in PARENT_NEIGHBOURHOODS[parent_key]:\n",
    "            sub_neighbourhood_index = neighbourhoods_data.index[neighbourhoods_data[NEIGHBOURHOOD_COL_KEY] == sub_neighbourhood].tolist()[0]\n",
    "            indexes_to_remove.append(sub_neighbourhood_index)\n",
    "            y_predict_sub_neighbourhood_value = get_non_negative_value(y_predict[sub_neighbourhood_index])\n",
    "            y_predict_parent_value = y_predict_parent_value + y_predict_sub_neighbourhood_value\n",
    "            y_data_sub_neighbourhood_value = get_non_negative_value(y_data[sub_neighbourhood_index])\n",
    "            y_data_parent_value = y_data_parent_value + y_data_sub_neighbourhood_value\n",
    "        y_predict[parent_index] = y_predict_parent_value\n",
    "        y_data[parent_index] = y_data_parent_value\n",
    "    neighbourhoods_data = neighbourhoods_data.drop(neighbourhoods_data.index[indexes_to_remove])\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for index in indexes_to_remove:\n",
    "        y_predict = np.delete(y_predict,index)\n",
    "        y_data = np.delete(y_data,index)\n",
    "    return y_data,y_predict,neighbourhoods_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluation <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent method for calculating all hotspot performance metrics\n",
    "def calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, prediction_neighbourhoods,num_hotspots):\n",
    "    total_predictions = len(y_predict)\n",
    "    predicted_hotspots, actual_hotspots = determine_hotspots(y_actual, \n",
    "                                                             actual_neighbourhoods, \n",
    "                                                             prediction_neighbourhoods,\n",
    "                                                             num_hotspots)\n",
    "    num_predictions = len(predicted_hotspots)\n",
    "    classification_scores = calculate_standard_scores(num_predictions,\n",
    "                                                      predicted_hotspots,\n",
    "                                                      actual_hotspots,\n",
    "                                                      total_predictions)\n",
    "    misclassification_scores = get_missed_incidents(predicted_hotspots, \n",
    "                                                    actual_hotspots, \n",
    "                                                    y_actual, \n",
    "                                                    y_predict,\n",
    "                                                    num_predictions,\n",
    "                                                    actual_neighbourhoods)\n",
    "    return classification_scores + misclassification_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate misclassification severity based on the number of incidents missed\n",
    "def get_missed_incidents(predicted_hotspots, actual_hotspots, y_actual, y_predict,num_predictions,actual_neighbourhoods):\n",
    "    incidents_correct = 0\n",
    "    incidents_missed = 0\n",
    "    additional_incidents_caught = 0\n",
    "    for neighbourhood in predicted_hotspots:\n",
    "        index = np.where(actual_neighbourhoods == neighbourhood)[0]\n",
    "        value = y_actual[index]\n",
    "        if neighbourhood in actual_hotspots:\n",
    "            incidents_correct += value\n",
    "        else:\n",
    "            additional_incidents_caught += value\n",
    "    i = 0\n",
    "    i_limit = 0\n",
    "    lowest_actual = y_actual[num_predictions-1]\n",
    "    while i < num_predictions:\n",
    "        if y_actual[i] != lowest_actual:\n",
    "            neighbourhood = actual_hotspots[i]\n",
    "            if neighbourhood not in predicted_hotspots:\n",
    "                index = np.where(actual_neighbourhoods == neighbourhood)[0]\n",
    "                value = y_actual[index]\n",
    "                incidents_missed += value\n",
    "            i += 1\n",
    "        else:\n",
    "            i_limit = i\n",
    "            i = num_predictions\n",
    "    i = i_limit\n",
    "    remaining_hotspots_to_find = num_predictions - i\n",
    "    hotspots_found = 0\n",
    "    while i < len(actual_hotspots):\n",
    "        neighbourhood = actual_hotspots[i]\n",
    "        if neighbourhood in predicted_hotspots:\n",
    "            hotspots_found += 1\n",
    "        i += 1\n",
    "    num_lowest_value_hotspots_missing = remaining_hotspots_to_find - hotspots_found\n",
    "    lowest_values_missed = lowest_actual * num_lowest_value_hotspots_missing\n",
    "    incidents_missed += lowest_values_missed\n",
    "    total_actual_hotspots = incidents_correct + incidents_missed\n",
    "    net_missed = incidents_missed - additional_incidents_caught\n",
    "    misclassification_severity = 0.0\n",
    "    if net_missed != 0:\n",
    "        misclassification_severity = (net_missed / total_actual_hotspots)[0]\n",
    "    return [misclassification_severity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine hotspots based on predicted values\n",
    "def determine_hotspots(y_actual, actual_neighbourhoods, prediction_neighbourhoods,num_hotspots):\n",
    "    #sorted numpy arrays expected \n",
    "    predicted_hotspots = prediction_neighbourhoods[:num_hotspots]\n",
    "    actual_hotspots = actual_neighbourhoods[:num_hotspots]\n",
    "    lowest_hotspot_value = y_actual[num_hotspots-1]\n",
    "    i = num_hotspots\n",
    "    while i < len(actual_neighbourhoods):\n",
    "        if y_actual[i] == lowest_hotspot_value:\n",
    "            np.append(actual_hotspots,actual_neighbourhoods[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            i = len(actual_neighbourhoods)\n",
    "    return predicted_hotspots, actual_hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate standard accuracy scores for classification\n",
    "def calculate_standard_scores(num_predictions,predicted_hotspots,actual_hotspots,total_predictions):\n",
    "    #true positives\n",
    "    tp = 0\n",
    "    #false positives\n",
    "    fp = 0\n",
    "    for i in range(num_predictions):\n",
    "        if predicted_hotspots[i] in actual_hotspots:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp +=1\n",
    "    #true negatives\n",
    "    tn = total_predictions-num_predictions-fp\n",
    "    #false negatives\n",
    "    fn = fp\n",
    "    #sensitivity/recall\n",
    "    sensitivity = 0\n",
    "    if tp + fn != 0:\n",
    "        sensitivity = tp / (tp + fn)\n",
    "    #specificity\n",
    "    specificity = 1\n",
    "    if tn + fp !=0:\n",
    "        specificity = tn / (tn + fp)\n",
    "    #precision\n",
    "    precision = 0\n",
    "    if tp + fp !=0:\n",
    "        precision = tp / (tp + fp)\n",
    "    #f1 - incorporates both sensitivity/recall and precision\n",
    "    f1 = 0\n",
    "    if precision + sensitivity != 0:\n",
    "        f1 = round(2 * (precision * sensitivity) / (precision + sensitivity),5)\n",
    "    #matthews correlation coefficient\n",
    "    mcc = 1\n",
    "    if sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn+fn)) != 0:\n",
    "        mcc = round(((tp * tn) - (fp * fn)) / (sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn+fn))),5)\n",
    "    return [sensitivity, specificity, precision, f1, mcc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scores for all models\n",
    "def get_results(num_hotspots, first_date, last_date, df):\n",
    "    all_scores = {}\n",
    "    for score_key in SCORE_KEYS:\n",
    "        all_scores[score_key] = []\n",
    "    algorithm_names = []\n",
    "    for model_key in MODELS:\n",
    "        for feature_key in FEATURES:\n",
    "            scores = score_algorithm(model_key, feature_key,num_hotspots, first_date, last_date, df)\n",
    "            average_scores = calculate_averages(scores)\n",
    "            for score_key in SCORE_KEYS:\n",
    "                all_scores[score_key].append(average_scores[score_key])\n",
    "            algorithm_names.append(model_key + \"\\n\" + feature_key)\n",
    "    return all_scores, algorithm_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average scores\n",
    "def calculate_averages(scores):\n",
    "    averages = {}\n",
    "    for score_key in SCORE_KEYS:\n",
    "        if len(scores[score_key]) == 0:\n",
    "            averages[score_key] = 0\n",
    "        else:\n",
    "            averages[score_key] = sum(scores[score_key]) / len (scores[score_key])\n",
    "    return averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score specific algorithm for all dates in ranges\n",
    "def score_algorithm(model_key, feature_key,num_hotspots, first_date, last_date, df):\n",
    "    all_scores = {}\n",
    "    for score_key in SCORE_KEYS:\n",
    "        all_scores[score_key] = []\n",
    "    while first_date <= last_date:\n",
    "        y_actual, actual_neighbourhoods, y_predict, prediction_neighbourhoods = get_hotspots(df, \n",
    "                                                                                model_key, \n",
    "                                                                                feature_key, \n",
    "                                                                                str(first_date.strftime(\"%d/%m/%Y\")))\n",
    "        results = calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, prediction_neighbourhoods,num_hotspots)\n",
    "        for i in range(len(SCORE_KEYS)):\n",
    "            all_scores[SCORE_KEYS[i]].append(results[i])\n",
    "        first_date += timedelta(days=1)\n",
    "    return all_scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best x scores from all scores of all models\n",
    "def best_x_scores(score_metric,x,algorithms,results,num_hotspots):\n",
    "    all_labels = get_labels()\n",
    "    x_labels = []\n",
    "    best_scores = []\n",
    "    scores = results[score_metric].copy()\n",
    "    num_scores = len(scores)\n",
    "    x = validate_x(num_scores,x)\n",
    "    i = 0\n",
    "    while i < x:\n",
    "        if HIGHEST_IS_BEST[score_metric]:\n",
    "            index = np.argmax(scores)\n",
    "        else:\n",
    "            index = np.argmin(scores)\n",
    "        x_labels.append(all_labels[index])\n",
    "        best_scores.append(scores[index])\n",
    "        all_labels = np.delete(all_labels,index)\n",
    "        scores = np.delete(scores,index)\n",
    "        i+=1\n",
    "    y_select = np.arange(len(best_scores))\n",
    "    plt.figure(figsize = (15,6))\n",
    "    ticks = plt.xticks(y_select,x_labels)\n",
    "    #plt.xticks(rotation=45)\n",
    "    title = plt.title(\"Top \" + str(x) + \" \" + score_metric + \" scores\\nwhen predicting the top \" \n",
    "              + str(num_hotspots) + \" Crime Hotspots\")\n",
    "    x_label = plt.xlabel(\"Algorithm\")\n",
    "    y_label = plt.ylabel(score_metric + \" score\")\n",
    "    high_score = max(best_scores)\n",
    "    low_score = min(best_scores)\n",
    "    min_y = low_score - 0.2 * (high_score - low_score) #0.5\n",
    "    max_y = high_score + 0.2 * (high_score - low_score) #0.5\n",
    "    plt.ylim(bottom = min_y, top = max_y)\n",
    "    bar = plt.bar(y_select, best_scores)\n",
    "    return bar, ticks, title, x_label, y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get chart labels\n",
    "def get_labels():\n",
    "    labels = []\n",
    "    for algorithm in algorithms:\n",
    "        for sel_method in sel_methods:\n",
    "            labels.append(algorithm_display_names[algorithms.index(algorithm)] + \"\\nusing\\n\" + \n",
    "                          feature_select_display_names[np.nonzero(sel_methods == sel_method)[0][0]] + \"\\ndataset\")\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return non negative value, no larger than number of results, where necessary\n",
    "def validate_x(num_scores,x):\n",
    "    soft_cap = 15\n",
    "    if num_scores < 0:\n",
    "        return 0\n",
    "    if x > num_scores or x < 1:\n",
    "        if num_scores > soft_cap:\n",
    "            return soft_cap\n",
    "        else:\n",
    "            return num_scores\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main method to commence evaluation of all models\n",
    "def evaluate_models(num_hotspots_range, top_x, df):\n",
    "    first_date = datetime.strptime(df['Date'].iloc[0], '%d/%m/%Y')\n",
    "    last_date = datetime.strptime(df['Date'].iloc[len(df) - 1], '%d/%m/%Y')\n",
    "    bars = []\n",
    "    for i in num_hotspots_range:\n",
    "        num_hotspots = i\n",
    "        results, algorithms = get_results(num_hotspots, first_date, last_date, df)\n",
    "        for metric in SCORE_KEYS:\n",
    "            bars.append(best_x_scores(metric,top_x,algorithms,results,num_hotspots))\n",
    "    return bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENSITIVITY_KEY = \"Sensitivity\"\n",
    "SPECIFICITY_KEY = \"Specificity\"\n",
    "PRECISION_KEY = \"Precision\"\n",
    "F1_KEY = \"F1\"\n",
    "MCC_KEY = \"MCC\"\n",
    "SEVERITY_KEY = \"Lowest Misclassification Severity\"\n",
    "SCORE_KEYS = [\n",
    "    SENSITIVITY_KEY,\n",
    "    SPECIFICITY_KEY,\n",
    "    PRECISION_KEY,\n",
    "    F1_KEY,\n",
    "    MCC_KEY,\n",
    "    SEVERITY_KEY\n",
    "]\n",
    "HIGHEST_IS_BEST = {\n",
    "    SENSITIVITY_KEY: True,\n",
    "    SPECIFICITY_KEY: True,\n",
    "    PRECISION_KEY: True,\n",
    "    F1_KEY:True,\n",
    "    MCC_KEY: True,\n",
    "    SEVERITY_KEY: False\n",
    "}\n",
    "\n",
    "num_hotspots_range = [5,10,15,20]\n",
    "top_x = 10\n",
    "df = open_file(GUI_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bars = evaluate_models(num_hotspots_range,top_x, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> In depth analysis required as no clear best algorithm <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine scores for all given hotspot ranges\n",
    "def evaluate_models_in_depth(num_hotspots_range, top_x, df):\n",
    "    first_date = datetime.strptime(df['Date'].iloc[0], '%d/%m/%Y')\n",
    "    last_date = datetime.strptime(df['Date'].iloc[len(df) - 1], '%d/%m/%Y')\n",
    "    all_results = {}\n",
    "    result_names = {}\n",
    "    for i in num_hotspots_range:\n",
    "        num_hotspots = i\n",
    "        results, algorithms = get_results(num_hotspots, first_date, last_date, df)\n",
    "        all_results [str(i)] = {}\n",
    "        result_names [str(i)] = {}\n",
    "        for metric in SCORE_KEYS:\n",
    "            all_results[str(i)][str(metric)], result_names[str(i)][str(metric)] = best_x_scores_no_graph(metric,\n",
    "                                                                                                         top_x,algorithms,\n",
    "                                                                                                         results,num_hotspots)\n",
    "    return all_results, result_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine top x algorithms without generating a graph\n",
    "def best_x_scores_no_graph(score_metric,x,algorithms,results,num_hotspots):\n",
    "    labels = get_labels()\n",
    "    all_labels = []\n",
    "    for i in range(0,len(labels)):\n",
    "        all_labels.append(labels[i].replace('\\n',' '))\n",
    "    x_labels = []\n",
    "    best_scores = []\n",
    "    best_names = []\n",
    "    scores = results[score_metric].copy()\n",
    "    num_scores = len(scores)\n",
    "    x = validate_x(num_scores,x)\n",
    "    i = 0\n",
    "    while i < x:\n",
    "        if HIGHEST_IS_BEST[score_metric]:\n",
    "            index = np.argmax(scores)\n",
    "        else:\n",
    "            index = np.argmin(scores)\n",
    "        x_labels.append(all_labels[index])\n",
    "        best_scores.append(scores[index])\n",
    "        best_names.append(all_labels[index])\n",
    "        all_labels = np.delete(all_labels,index)\n",
    "        scores = np.delete(scores,index)\n",
    "        i+=1\n",
    "    return best_scores, best_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get in depth results for hotspot ranges 1-40\n",
    "num_hotspots_range = list(range(1,41))\n",
    "top_x = 10\n",
    "df = open_file(GUI_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results, names = evaluate_models_in_depth(num_hotspots_range,top_x, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an analysis of the top 5 algorithms for every range\n",
    "def get_full_analysis(all_results, all_names, num_hotspots_range):\n",
    "    #define table headings\n",
    "    num_hotspot_predictions = []\n",
    "    placings = [[], [], [], [], []]\n",
    "    best_scores = []\n",
    "    for i in num_hotspots_range:\n",
    "        num_hotspots_key = str(i)\n",
    "        results = all_results[num_hotspots_key]['Lowest Misclassification Severity']\n",
    "        names = all_names[num_hotspots_key]['Lowest Misclassification Severity']\n",
    "        num_hotspot_predictions.append(num_hotspots_key)\n",
    "        best_scores.append(results[0])\n",
    "        for j in range (0, len(placings)):\n",
    "            placings[j].append(names[j])\n",
    "    table = pd.DataFrame()\n",
    "    table['Number of Hotspots Predicted'] = num_hotspots_range\n",
    "    table['Best Score'] = [round(num,2) for num in best_scores]\n",
    "    table['Rank 1'] = placings[0]\n",
    "    table['Rank 2'] = placings[1]\n",
    "    table['Rank 3'] = placings[2]\n",
    "    table['Rank 4'] = placings[3]\n",
    "    table['Rank 5'] = placings[4]\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the score of the given rank\n",
    "def get_score_by_rank(scores, target_rank):\n",
    "    rank = 1\n",
    "    previous_score = scores[0]\n",
    "    for i in range(0, len(scores)):\n",
    "        if scores[i] != previous_score:\n",
    "            rank += 1\n",
    "            previous_score = scores[i]\n",
    "        if rank == target_rank:\n",
    "            return scores[i]\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average scores for every hotspot range\n",
    "def get_average_score_per_num_hotspots(algorithms, results, names, num_hotspots_range):\n",
    "    average_scores = []\n",
    "    all_scores = {}\n",
    "    for algorithm in algorithms:\n",
    "        all_scores[algorithm] = []\n",
    "    for i in num_hotspots_range:\n",
    "        for algorithm in algorithms:\n",
    "            if algorithm in names[str(i)]['Lowest Misclassification Severity']:\n",
    "                index = names[str(i)]['Lowest Misclassification Severity'].index(algorithm)\n",
    "                all_scores[algorithm].append(results[str(i)]['Lowest Misclassification Severity'][index])\n",
    "    for key in all_scores:\n",
    "        scores = all_scores[key]\n",
    "        average_scores.append(sum(scores) / len (scores))\n",
    "    return average_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the number of times an algorithm ranked first, and which hotspot ranges that was for\n",
    "def get_best_counts(all_results, all_names, num_hotspots_range, rank):\n",
    "    algorithms = []\n",
    "    times_first = []\n",
    "    hotspot_xs = []\n",
    "    table_arrays = [algorithms, times_first, hotspot_xs]\n",
    "    for i in num_hotspots_range:\n",
    "        num_hotspots_key = str(i)\n",
    "        results = all_results[num_hotspots_key]['Lowest Misclassification Severity']\n",
    "        names = all_names[num_hotspots_key]['Lowest Misclassification Severity']\n",
    "        best_score = get_score_by_rank(results, rank)\n",
    "        for j in range (0, len(results)):\n",
    "            algorithm = names[j]\n",
    "            score = results[j]\n",
    "            if score == best_score:\n",
    "                if algorithm not in algorithms:\n",
    "                    algorithms.append(algorithm)\n",
    "                    times_first.append(0)\n",
    "                    hotspot_xs.append(\"\")\n",
    "                index = algorithms.index(algorithm)\n",
    "                times_first[index] = times_first[index] + 1\n",
    "                hotspot_xs[index] = hotspot_xs[index] + str(i) + \" \"\n",
    "    average_scores = get_average_score_per_num_hotspots(table_arrays[0], all_results, all_names, num_hotspots_range)\n",
    "    table = pd.DataFrame()\n",
    "    table['Algorithm'] = table_arrays[0]\n",
    "    table['Average Score'] = get_average_score_per_num_hotspots(table_arrays[0], all_results, all_names, num_hotspots_range)\n",
    "    #table = pd.concat([table,average_scores],axis=1,join = \"inner\")\n",
    "    table['Times Ranked ' + str(rank)] = [round (num,0) for num in table_arrays[1]]\n",
    "    table['Hotspot Values'] = table_arrays[2]\n",
    "    table = table.sort_values(by=['Times Ranked ' + str(rank)], ascending=False)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_analysis = get_full_analysis(results, names, num_hotspots_range)\n",
    "full_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_counts = get_best_counts(results, names, num_hotspots_range, 1)\n",
    "best_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Testing starts here <h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test open_file <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_open_file(file_path,expected_length,expected_columns):\n",
    "    df = open_file(file_path)\n",
    "    assert len(df) == expected_length, \"Data frame length not as expected.\"\n",
    "    actual_columns = df.columns    \n",
    "    assert len(actual_columns) == len(expected_columns), \"Actual columns not as expected.\"\n",
    "    for i in range(len(expected_columns)):\n",
    "        assert expected_columns[i] == actual_columns[i], \"Expected column \" + expected_columns[i] + \" but got \" + actual_columns[i]\n",
    "    print(\"All tests completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all columns of parent datafile\n",
    "def get_all_columns():\n",
    "    return ['Reports 1 day ago','Reports 2 days ago','Reports 3 days ago','Reports 4 days ago','Reports 5 days ago',\n",
    "                    'Reports 6 days ago','Reports 7 days ago','Reports 14 days ago','Reports 30 days ago',\n",
    "                    'Reports 365 days ago','Last 7 days reports','Last 14 days reports','Last 28 days reports',\n",
    "                    'Number of businesses','Businesses 1 day ago','Businesses 2 days ago','Businesses 3 days ago',\n",
    "                    'Businesses 4 days ago','Businesses 5 days ago','Businesses 6 days ago','Businesses 7 days ago',\n",
    "                    'Businesses 14 days ago','Businesses 30 days ago','Businesses 365 days ago','Number of closures',\n",
    "                    'Closures 1 day ago','Closures 2 days ago','Closures 3 days ago','Closures 4 days ago',\n",
    "                    'Closures 5 days ago','Closures 6 days ago','Closures 7 days ago','Closures 14 days ago',\n",
    "                    'Closures 30 days ago','Closures 365 days ago','Last 7 days closures','Last 14 days closures',\n",
    "                    'Last 28 days closures','Number of openings','Openings 1 day ago','Openings 2 days ago',\n",
    "                    'Openings 3 days ago','Openings 4 days ago','Openings 5 days ago','Openings 6 days ago',\n",
    "                    'Openings 7 days ago','Openings 14 days ago','Openings 30 days ago','Openings 365 days ago',\n",
    "                    'Last 7 days openings','Last 14 days openings','Last 28 days openings','Alamo Square','Anza Vista',\n",
    "                    'Apparel City','Aquatic Park / Ft. Mason','Balboa Terrace','Bayview','Bernal Heights','Bret Harte',\n",
    "                    'Buena Vista','Candlestick Point SRA','Castro','Cathedral Hill','Cayuga','Central Waterfront','Chinatown',\n",
    "                    'Civic Center','Clarendon Heights','Cole Valley','Corona Heights','Cow Hollow','Crocker Amazon',\n",
    "                    'Diamond Heights','Dogpatch','Dolores Heights','Downtown / Union Square','Duboce Triangle','Eureka Valley',\n",
    "                    'Excelsior','Fairmount','Financial District','Fishermans Wharf','Forest Hill','Forest Knolls','Glen Park',\n",
    "                    'Golden Gate Heights','Golden Gate Park','Haight Ashbury','Hayes Valley','Holly Park','Hunters Point',\n",
    "                    'India Basin','Ingleside','Ingleside Terraces','Inner Richmond','Inner Sunset','Japantown','Laguna Honda',\n",
    "                    'Lake Street','Lakeshore','Laurel Heights / Jordan Park','Lincoln Park / Ft. Miley','Little Hollywood',\n",
    "                    'Lone Mountain','Lower Haight','Lower Nob Hill','Lower Pacific Heights','Marina','McLaren Park',\n",
    "                    'Merced Heights','Merced Manor','Midtown Terrace','Mint Hill','Miraloma Park','Mission','Mission Bay',\n",
    "                    'Mission Dolores','Mission Terrace','Monterey Heights','Mt. Davidson Manor','Nob Hill','Noe Valley',\n",
    "                    'North Beach','Northern Waterfront','Oceanview','Outer Mission','Outer Richmond','Outer Sunset',\n",
    "                    'Pacific Heights','Panhandle','Parkmerced','Parkside','Parnassus Heights','Peralta Heights',\n",
    "                    'Polk Gulch','Portola','Potrero Hill','Presidio Heights','Presidio National Park','Presidio Terrace',\n",
    "                    'Produce Market','Rincon Hill','Russian Hill','Seacliff','Sherwood Forest','Showplace Square',\n",
    "                    'Silver Terrace','South Beach','South of Market','St. Francis Wood','St. Marys Park','Stonestown',\n",
    "                    'Sunnydale','Sunnyside','Sutro Heights','Telegraph Hill','Tenderloin','Treasure Island','Union Street',\n",
    "                    'University Mound','Upper Market','Visitacion Valley','West Portal','Western Addition',\n",
    "                    'Westwood Highlands','Westwood Park','Yerba Buena Island','Friday','Saturday','Sunday','Thursday',\n",
    "                    'Tuesday','Wednesday','Todays Reports','Date','Neighborhood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all neighbourhoods\n",
    "def get_all_neighbourhoods():\n",
    "    return ['Alamo Square','Anza Vista', 'Apparel City','Aquatic Park / Ft. Mason','Ashbury Heights','Balboa Terrace','Bayview',\n",
    "            'Bernal Heights','Bret Harte','Buena Vista','Candlestick Point SRA','Castro','Cathedral Hill','Cayuga',\n",
    "            'Central Waterfront','Chinatown','Civic Center','Clarendon Heights','Cole Valley','Corona Heights','Cow Hollow',\n",
    "            'Crocker Amazon','Diamond Heights','Dogpatch','Dolores Heights','Downtown / Union Square','Duboce Triangle',\n",
    "            'Eureka Valley','Excelsior','Fairmount','Financial District','Fishermans Wharf','Forest Hill','Forest Knolls',\n",
    "            'Glen Park','Golden Gate Heights','Golden Gate Park','Haight Ashbury','Hayes Valley','Holly Park','Hunters Point',\n",
    "            'India Basin','Ingleside','Ingleside Terraces','Inner Richmond','Inner Sunset','Japantown','Laguna Honda',\n",
    "            'Lake Street','Lakeshore','Laurel Heights / Jordan Park','Lincoln Park / Ft. Miley','Little Hollywood',\n",
    "            'Lone Mountain','Lower Haight','Lower Nob Hill','Lower Pacific Heights','Marina','McLaren Park',\n",
    "            'Merced Heights','Merced Manor','Midtown Terrace','Mint Hill','Miraloma Park','Mission','Mission Bay',\n",
    "            'Mission Dolores','Mission Terrace','Monterey Heights','Mt. Davidson Manor','Nob Hill','Noe Valley',\n",
    "            'North Beach','Northern Waterfront','Oceanview','Outer Mission','Outer Richmond','Outer Sunset',\n",
    "            'Pacific Heights','Panhandle','Parkmerced','Parkside','Parnassus Heights','Peralta Heights',\n",
    "            'Polk Gulch','Portola','Potrero Hill','Presidio Heights','Presidio National Park','Presidio Terrace',\n",
    "            'Produce Market','Rincon Hill','Russian Hill','Seacliff','Sherwood Forest','Showplace Square',\n",
    "            'Silver Terrace','South Beach','South of Market','St. Francis Wood','St. Marys Park','Stonestown',\n",
    "            'Sunnydale','Sunnyside','Sutro Heights','Telegraph Hill','Tenderloin','Treasure Island','Union Street',\n",
    "            'University Mound','Upper Market','Visitacion Valley','West Portal','Western Addition',\n",
    "            'Westwood Highlands','Westwood Park','Yerba Buena Island']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all 1\n",
    "def get_all_ones():\n",
    "    all_ones = []\n",
    "    for i in range(0,117):\n",
    "        all_ones.append(1)\n",
    "    return all_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test successful load\n",
    "expected_columns = get_all_columns()\n",
    "file_path = \"tuning_test_data\"\n",
    "expected_length = 819\n",
    "test_open_file(file_path,expected_length,expected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test failed load\n",
    "expected_columns = []\n",
    "file_path = \"incorrect_path\"\n",
    "expected_length = 0\n",
    "test_open_file(file_path,expected_length,expected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test get_hotspots <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_hotspots(test_model_name,test_feature_name,calendar_date, expected_y_actual, expected_actual_neighbourhoods,\n",
    "                     expected_y_predict, expected_prediction_neighbourhoods, file_name):\n",
    "    data = open_file(file_name)\n",
    "    y_actual, actual_neighbourhoods, y_predict, prediction_neighbourhoods = get_hotspots(data, \n",
    "                                                                                     test_model_name, \n",
    "                                                                                     test_feature_name,\n",
    "                                                                                     calendar_date)\n",
    "    for i in range(0,len(y_actual)):\n",
    "        assert y_actual[i] == expected_y_actual[i], \"Y actual not as expected.\"\n",
    "    for i in range(0,len(expected_y_actual)):\n",
    "        assert expected_y_actual[i] == y_actual[i], \"Y actual not as expected.\"\n",
    "        \n",
    "    for i in range(0,len(y_predict)):\n",
    "        assert y_predict[i] == expected_y_predict[i], \"Y actual not as expected.\"\n",
    "    for i in range(0,len(expected_y_predict)):\n",
    "        assert expected_y_predict[i] == y_predict[i], \"Y actual not as expected.\"   \n",
    "    for i in range(0,len(actual_neighbourhoods)):\n",
    "        assert actual_neighbourhoods[i] == expected_actual_neighbourhoods[i], \"Actual neighbourhoods not as expected.\"\n",
    "    for i in range(0,len(expected_actual_neighbourhoods)):\n",
    "        assert expected_actual_neighbourhoods[i] == actual_neighbourhoods[i], \"Actual neighbourhoods not as expected.\"    \n",
    "    for i in range(0,len(prediction_neighbourhoods)):\n",
    "        assert prediction_neighbourhoods[i] == expected_prediction_neighbourhoods[i], \"Predicted neighbourhoods not as expected.\"  \n",
    "    for i in range(0,len(expected_prediction_neighbourhoods)):\n",
    "        assert expected_prediction_neighbourhoods[i] == prediction_neighbourhoods[i], \"Predicted neighbourhoods not as expected.\"\n",
    "    print(\"Tests completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"tuning_test_data\"\n",
    "test_model_name = \"Tuning Template Test\"\n",
    "test_model_file_tag = \"tuning_template_test_model\"\n",
    "test_feature_file_tag = \"arbitrary_name\"\n",
    "test_feature_name = \"Arbitrary Feature Name\"\n",
    "test_feature_features = ['Reports 1 day ago', 'Reports 2 days ago', 'Reports 3 days ago',\n",
    "                       'Reports 4 days ago', 'Reports 5 days ago', 'Reports 6 days ago',\n",
    "                      'Reports 7 days ago','Reports 14 days ago','Reports 30 days ago','Reports 365 days ago']\n",
    "FEATURES[test_feature_name] = test_feature_features\n",
    "FEATURE_FILE_TAGS[test_feature_name] = test_feature_file_tag\n",
    "FEATURE_NAMES_BY_FILE_TAG[test_feature_file_tag] = test_feature_name\n",
    "MODEL_FILE_TAGS[test_model_name] = test_model_file_tag\n",
    "calendar_date = \"05/01/2021\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_y_actual = [5, 4, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "expected_actual_neighbourhoods = ['Downtown / Union Square', 'Bernal Heights', 'Western Addition', 'Portola',\n",
    " 'Bayview', 'Eureka Valley', 'South Beach', 'Lincoln Park / Ft. Miley',\n",
    " 'Hunters Point', 'Potrero Hill', 'Presidio Heights', 'Cow Hollow',\n",
    " 'Russian Hill', 'Cole Valley', 'Mission Terrace', 'Hayes Valley',\n",
    " 'Stonestown', 'Central Waterfront', 'Northern Waterfront', 'Buena Vista',\n",
    " 'Visitacion Valley', 'North Beach', 'Forest Hill', 'South of Market',\n",
    " 'Haight Ashbury', 'Golden Gate Park', 'Yerba Buena Island',\n",
    " 'Golden Gate Heights', 'Ingleside Terraces', 'Inner Richmond',\n",
    " 'Inner Sunset', 'Lake Street', 'Lakeshore', 'Ingleside', 'Diamond Heights',\n",
    " 'Glen Park', 'Forest Knolls', 'Financial District', 'Excelsior',\n",
    " 'Duboce Triangle', 'Crocker Amazon', 'Corona Heights', 'Clarendon Heights',\n",
    " 'Civic Center', 'Candlestick Point SRA', 'Balboa Terrace', 'Anza Vista',\n",
    " 'Laurel Heights / Jordan Park', 'Merced Manor', 'Little Hollywood',\n",
    " 'St. Francis Wood', 'Panhandle', 'Parkside', 'Presidio National Park',\n",
    " 'Seacliff', 'Sherwood Forest', 'Silver Terrace', 'Sunnyside', 'Outer Sunset',\n",
    " 'Telegraph Hill', 'Tenderloin', 'Treasure Island', 'Upper Market',\n",
    " 'West Portal', 'Westwood Highlands', 'Pacific Heights', 'Outer Richmond',\n",
    " 'Lone Mountain', 'Mission', 'Lower Pacific Heights', 'Marina',\n",
    " 'Merced Heights', 'Westwood Park', 'Midtown Terrace', 'Miraloma Park',\n",
    " 'Mission Bay', 'Outer Mission', 'Mission Dolores', 'Monterey Heights',\n",
    " 'Mt. Davidson Manor', 'Nob Hill', 'Noe Valley', 'Oceanview', 'Alamo Square']\n",
    "expected_y_predict = expected_y_actual\n",
    "expected_prediction_neighbourhoods = expected_actual_neighbourhoods\n",
    "test_get_hotspots(test_model_name,test_feature_name,calendar_date, expected_y_actual, expected_actual_neighbourhoods,\n",
    "                     expected_y_predict, expected_prediction_neighbourhoods, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test load_model <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_model(x_data, y_data, neighbourhoods_data, model_key, features_key):\n",
    "    y_actual, actual_neighbourhoods, y_predict, prediction_neighbourhoods = load_model(x_data, \n",
    "                                                                                       y_data, \n",
    "                                                                                       neighbourhoods_data,\n",
    "                                                                                       model_key, \n",
    "                                                                                       features_key)\n",
    "    for i in range(0,len(y_actual)):\n",
    "        assert y_actual[i] == expected_y_actual[i], \"Y actual not as expected.\"\n",
    "    for i in range(0,len(expected_y_actual)):\n",
    "        assert expected_y_actual[i] == y_actual[i], \"Y actual not as expected.\"   \n",
    "    for i in range(0,len(y_predict)):\n",
    "        assert y_predict[i] == expected_y_predict[i], \"Y actual not as expected.\"\n",
    "    for i in range(0,len(expected_y_predict)):\n",
    "        assert expected_y_predict[i] == y_predict[i], \"Y actual not as expected.\"   \n",
    "    for i in range(0,len(actual_neighbourhoods)):\n",
    "        assert actual_neighbourhoods[i] == expected_actual_neighbourhoods[i], \"Actual neighbourhoods not as expected.\"\n",
    "    for i in range(0,len(expected_actual_neighbourhoods)):\n",
    "        assert expected_actual_neighbourhoods[i] == actual_neighbourhoods[i], \"Actual neighbourhoods not as expected.\"    \n",
    "    for i in range(0,len(prediction_neighbourhoods)):\n",
    "        assert prediction_neighbourhoods[i] == expected_prediction_neighbourhoods[i], \"Predicted neighbourhoods not as expected.\"  \n",
    "    for i in range(0,len(expected_prediction_neighbourhoods)):\n",
    "        assert expected_prediction_neighbourhoods[i] == prediction_neighbourhoods[i], \"Predicted neighbourhoods not as expected.\"\n",
    "    print(\"Tests completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_ones = get_all_ones()\n",
    "x_columns = ['Reports 1 day ago','Reports 2 days ago','Reports 3 days ago','Reports 4 days ago',\n",
    "                      'Reports 5 days ago','Reports 6 days ago','Reports 7 days ago','Reports 14 days ago',\n",
    "                      'Reports 30 days ago','Reports 365 days ago']\n",
    "x_data = pd.DataFrame()\n",
    "for col in x_columns:\n",
    "    x_data[col] = all_ones\n",
    "y_data = pd.DataFrame()\n",
    "y_data['Todays Reports'] = all_ones\n",
    "neighbourhoods_data = pd.DataFrame()\n",
    "neighbourhoods_data['Neighborhood'] = get_all_neighbourhoods()\n",
    "model_key = 'Tuning Template Test'\n",
    "features_key = 'Arbitrary Feature Name'\n",
    "test_load_model(x_data, y_data, neighbourhoods_data, model_key, features_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test make_prediction <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_make_prediction(model_path, x_data, y_data, neighbourhoods_data):\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    y_actual, actual_neighbourhoods, y_predict, prediction_neighbourhoods = make_prediction(model,\n",
    "                                                                                            x_data, \n",
    "                                                                                            y_data, \n",
    "                                                                                            neighbourhoods_data)\n",
    "    for i in range(0,len(y_actual)):\n",
    "        assert y_actual[i] == expected_y_actual[i], \"Y actual not as expected.\"\n",
    "    for i in range(0,len(expected_y_actual)):\n",
    "        assert expected_y_actual[i] == y_actual[i], \"Y actual not as expected.\"   \n",
    "    for i in range(0,len(y_predict)):\n",
    "        assert y_predict[i] == expected_y_predict[i], \"Y actual not as expected.\"\n",
    "    for i in range(0,len(expected_y_predict)):\n",
    "        assert expected_y_predict[i] == y_predict[i], \"Y actual not as expected.\"   \n",
    "    for i in range(0,len(actual_neighbourhoods)):\n",
    "        assert actual_neighbourhoods[i] == expected_actual_neighbourhoods[i], \"Actual neighbourhoods not as expected.\"\n",
    "    for i in range(0,len(expected_actual_neighbourhoods)):\n",
    "        assert expected_actual_neighbourhoods[i] == actual_neighbourhoods[i], \"Actual neighbourhoods not as expected.\"    \n",
    "    for i in range(0,len(prediction_neighbourhoods)):\n",
    "        assert prediction_neighbourhoods[i] == expected_prediction_neighbourhoods[i], \"Predicted neighbourhoods not as expected.\"  \n",
    "    for i in range(0,len(expected_prediction_neighbourhoods)):\n",
    "        assert expected_prediction_neighbourhoods[i] == prediction_neighbourhoods[i], \"Predicted neighbourhoods not as expected.\"\n",
    "    print(\"Tests completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"CrimeGUI/Models/tuning_template_test_model_arbitrary_name\"\n",
    "test_make_prediction(model_path,x_data, y_data, neighbourhoods_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test get non negative value <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_non_negative_value(nums_in,expected_outs):\n",
    "    for i in range(0,len(nums_in)):\n",
    "        result = get_non_negative_value(nums_in[i])\n",
    "        assert result == expected_outs[i], \"Returned value not as expected\"\n",
    "    print(\"Tests completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_in = [-10,-2,-1,0,1,2,10]\n",
    "expected_outs = [0,0,0,0,1,2,10]\n",
    "test_get_non_negative_value(nums_in,expected_outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test merge sub neighbourhoods <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_merge_sub_neighbourhoods(y_data_in,y_predict_in,neighbourhoods_data_in, expected_y_actual, expected_y_predict,\n",
    "                                 expected_neighbourhoods_data):\n",
    "    y_actual,y_predict,neighbourhoods_data = merge_sub_neighbourhoods(y_data_in,y_predict_in,neighbourhoods_data_in)\n",
    "    neighbourhoods_data.reset_index(drop=True, inplace=True)\n",
    "    for i in range(0,len(y_actual)):\n",
    "        assert y_actual[i] == expected_y_actual[i], \"Y actual not as expected.\"\n",
    "    for i in range(0,len(expected_y_actual)):\n",
    "        assert expected_y_actual[i] == y_actual[i], \"Y actual not as expected.\"   \n",
    "    for i in range(0,len(y_predict)):\n",
    "        assert y_predict[i] == expected_y_predict[i], \"Y actual not as expected.\"\n",
    "    for i in range(0,len(expected_y_predict)):\n",
    "        assert expected_y_predict[i] == y_predict[i], \"Y actual not as expected.\" \n",
    "    for i in range(0,len(neighbourhoods_data)):\n",
    "        assert neighbourhoods_data['Neighborhood'][i] == expected_neighbourhoods_data[i], \"Neighbourhoods not as expected.\"\n",
    "    for i in range(0,len(expected_neighbourhoods_data)):\n",
    "        assert expected_neighbourhoods_data[i] == neighbourhoods_data['Neighborhood'][i], \"Neighbourhoods not as expected.\"   \n",
    "    print(\"Tests completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_in = pd.DataFrame()\n",
    "y_data_in['Todays Reports'] = get_all_ones()\n",
    "y_predict_in = all_ones\n",
    "neighbourhoods_data_in = pd.DataFrame()\n",
    "neighbourhoods_data_in['Neighborhood'] = get_all_neighbourhoods()\n",
    "expected_y_actual = [1, 1, 1, 3, 4, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 5, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, \n",
    "                     2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 3, 2, 2, 1, 2, 1, 1, 1, 2,\n",
    "                     2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1]\n",
    "expected_y_predict = expected_y_actual\n",
    "expected_neighbourhoods_data = ['Alamo Square', 'Anza Vista', 'Balboa Terrace', 'Bayview', 'Bernal Heights', 'Buena Vista', \n",
    "                                'Candlestick Point SRA', 'Central Waterfront', 'Civic Center', 'Clarendon Heights', \n",
    "                                'Cole Valley', 'Corona Heights', 'Cow Hollow', 'Crocker Amazon', 'Diamond Heights', \n",
    "                                'Downtown / Union Square', 'Duboce Triangle', 'Eureka Valley', 'Excelsior', \n",
    "                                'Financial District', 'Forest Hill', 'Forest Knolls', 'Glen Park', 'Golden Gate Heights', \n",
    "                                'Golden Gate Park', 'Haight Ashbury', 'Hayes Valley', 'Hunters Point', 'Ingleside', \n",
    "                                'Ingleside Terraces', 'Inner Richmond', 'Inner Sunset', 'Lake Street', 'Lakeshore', \n",
    "                                'Laurel Heights / Jordan Park', 'Lincoln Park / Ft. Miley', 'Little Hollywood', \n",
    "                                'Lone Mountain', 'Lower Pacific Heights', 'Marina', 'Merced Heights', 'Merced Manor', \n",
    "                                'Midtown Terrace', 'Miraloma Park', 'Mission', 'Mission Bay', 'Mission Dolores', \n",
    "                                'Mission Terrace', 'Monterey Heights', 'Mt. Davidson Manor', 'Nob Hill', 'Noe Valley', \n",
    "                                'North Beach', 'Northern Waterfront', 'Oceanview', 'Outer Mission', 'Outer Richmond', \n",
    "                                'Outer Sunset', 'Pacific Heights', 'Panhandle', 'Parkside', 'Portola', 'Potrero Hill', \n",
    "                                'Presidio Heights', 'Presidio National Park', 'Russian Hill', 'Seacliff', 'Sherwood Forest', \n",
    "                                'Silver Terrace', 'South Beach', 'South of Market', 'St. Francis Wood', 'Stonestown', \n",
    "                                'Sunnyside', 'Telegraph Hill', 'Tenderloin', 'Treasure Island', 'Upper Market', \n",
    "                                'Visitacion Valley', 'West Portal', 'Western Addition', 'Westwood Highlands', 'Westwood Park', \n",
    "                                'Yerba Buena Island']\n",
    "test_merge_sub_neighbourhoods(y_data_in,y_predict_in,neighbourhoods_data_in, expected_y_actual, expected_y_predict,\n",
    "                                 expected_neighbourhoods_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test calculate accuracy <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, prediction_neighbourhoods, expected_scores\n",
    "                            ,num_hotspots):\n",
    "    \n",
    "    result = calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, prediction_neighbourhoods,num_hotspots)\n",
    "    #returns [sensitivity, specificity, precision, f1, mcc, misclassification_severity]\n",
    "    assert result [0] == expected_scores [0], \"Sensitivity not as expected.\"\n",
    "    assert result [1] == expected_scores [1], \"Specificity not as expected.\"\n",
    "    assert result [2] == expected_scores [2], \"Precision not as expected.\"\n",
    "    assert result [3] == expected_scores [3], \"F1 not as expected.\"\n",
    "    assert result [4] == expected_scores [4], \"MCC not as expected.\"\n",
    "    assert result [5] == expected_scores [5], \"Misclassification severity not as expected.\"\n",
    "    print (\"All tests completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_neighbourhoods(x, reverse):\n",
    "    neighbourhoods = []\n",
    "    for i in range(0,x):\n",
    "        neighbourhoods.append('Neighbourhood ' + str(i))\n",
    "    if reverse:\n",
    "        neighbourhoods.reverse()\n",
    "    return neighbourhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = open_file(file_name)\n",
    "test_model_name = \"Tuning Template Test\"\n",
    "test_feature_name = \"Arbitrary Feature Name\"\n",
    "calendar_date = \"05/01/2021\"\n",
    "expected_scores = [1,1,1,1,1,0]\n",
    "num_hotspots = 10\n",
    "y_actual, actual_neighbourhoods, y_predict, prediction_neighbourhoods = get_hotspots(data, \n",
    "                                                                                     test_model_name, \n",
    "                                                                                     test_feature_name,\n",
    "                                                                                     calendar_date)\n",
    "test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, prediction_neighbourhoods, expected_scores, num_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "y_predict = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(len(y_actual), reverse = False), dtype=object)\n",
    "predicted_neighbourhoods = np.array(get_x_neighbourhoods(len(y_predict), reverse = False), dtype=object)\n",
    "expected_scores = [1,1,1,1,1,0]\n",
    "num_hotspots = 10\n",
    "test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, predicted_neighbourhoods, expected_scores, num_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "y_predict = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(len(y_actual), reverse = True), dtype=object)\n",
    "predicted_neighbourhoods = np.array(get_x_neighbourhoods(len(y_predict), reverse = False), dtype=object)\n",
    "expected_scores = [1,1,1,1,1,0]\n",
    "num_hotspots = 10\n",
    "test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, predicted_neighbourhoods, expected_scores, num_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "y_predict = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(len(y_actual), reverse = False), dtype=object)\n",
    "predicted_neighbourhoods = np.array(get_x_neighbourhoods(len(y_predict), reverse = True), dtype=object)\n",
    "expected_scores = [1,1,1,1,1,0]\n",
    "num_hotspots = 10\n",
    "test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, predicted_neighbourhoods, expected_scores, num_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "y_predict = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(len(y_actual), reverse = True), dtype=object)\n",
    "predicted_neighbourhoods = np.array(get_x_neighbourhoods(len(y_predict), reverse = True), dtype=object)\n",
    "expected_scores = [1,1,1,1,1,0]\n",
    "num_hotspots = 10\n",
    "test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, predicted_neighbourhoods, expected_scores, num_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "one_to_ten = [1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(one_to_ten))\n",
    "y_predict = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(len(y_actual), reverse = False), dtype=object)\n",
    "predicted_neighbourhoods = np.array(get_x_neighbourhoods(len(y_predict), reverse = False), dtype=object)\n",
    "expected_scores = [1,1,1,1,1,0]\n",
    "num_hotspots = 10\n",
    "test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, predicted_neighbourhoods, expected_scores, num_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "one_to_ten = [1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "y_predict = np.ndarray((10,),buffer = np.array(one_to_ten))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(len(y_actual), reverse = False), dtype=object)\n",
    "predicted_neighbourhoods = np.array(get_x_neighbourhoods(len(y_predict), reverse = False), dtype=object)\n",
    "expected_scores = [1,1,1,1,1,0]\n",
    "num_hotspots = 10\n",
    "test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, predicted_neighbourhoods, expected_scores, num_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "one_to_ten = [1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(one_to_ten))\n",
    "y_predict = np.ndarray((10,),buffer = np.array(one_to_ten))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(len(y_actual), reverse = False), dtype=object)\n",
    "predicted_neighbourhoods = np.array(get_x_neighbourhoods(len(y_predict), reverse = False), dtype=object)\n",
    "expected_scores = [1,1,1,1,1,0]\n",
    "num_hotspots = 10\n",
    "test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, predicted_neighbourhoods, expected_scores, num_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "one_to_ten = [1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(one_to_ten))\n",
    "y_predict = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(len(y_actual), reverse = True), dtype=object)\n",
    "predicted_neighbourhoods = np.array(get_x_neighbourhoods(len(y_predict), reverse = False), dtype=object)\n",
    "expected_scores = [1,1,1,1,1,0]\n",
    "num_hotspots = 10\n",
    "test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, predicted_neighbourhoods, expected_scores, num_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "one_to_ten = [1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(one_to_ten))\n",
    "y_predict = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(len(y_actual), reverse = False), dtype=object)\n",
    "predicted_neighbourhoods = np.array(get_x_neighbourhoods(len(y_predict), reverse = True), dtype=object)\n",
    "expected_scores = [1,1,1,1,1,0]\n",
    "num_hotspots = 10\n",
    "test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, predicted_neighbourhoods, expected_scores, num_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "one_to_ten = [1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(one_to_ten))\n",
    "y_predict = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(len(y_actual), reverse = True), dtype=object)\n",
    "predicted_neighbourhoods = np.array(get_x_neighbourhoods(len(y_predict), reverse = True), dtype=object)\n",
    "expected_scores = [1,1,1,1,1,0]\n",
    "num_hotspots = 10\n",
    "test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, predicted_neighbourhoods, expected_scores, num_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "y_predict = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(len(y_actual), reverse = False), dtype=object)\n",
    "predicted_neighbourhoods = np.array(get_x_neighbourhoods(len(y_predict), reverse = False), dtype=object)\n",
    "expected_scores = [1,1,1,1,1,0]\n",
    "num_hotspots = 9\n",
    "test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, predicted_neighbourhoods, expected_scores, num_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "y_predict = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(len(y_actual), reverse = True), dtype=object)\n",
    "predicted_neighbourhoods = np.array(get_x_neighbourhoods(len(y_predict), reverse = False), dtype=object)\n",
    "expected_scores = [0.8888888888888888,0,0.8888888888888888,0.88889,-0.11111,0.16666666666666666]\n",
    "num_hotspots = 9\n",
    "test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, predicted_neighbourhoods, expected_scores, num_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "y_predict = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(len(y_actual), reverse = False), dtype=object)\n",
    "predicted_neighbourhoods = np.array(get_x_neighbourhoods(len(y_predict), reverse = True), dtype=object)\n",
    "expected_scores = [0.8888888888888888,0,0.8888888888888888,0.88889,-0.11111,0.16666666666666666]\n",
    "num_hotspots = 9\n",
    "test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, predicted_neighbourhoods, expected_scores, num_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "y_predict = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(len(y_actual), reverse = True), dtype=object)\n",
    "predicted_neighbourhoods = np.array(get_x_neighbourhoods(len(y_predict), reverse = True), dtype=object)\n",
    "expected_scores = [1,1,1,1,1,0]\n",
    "num_hotspots = 9\n",
    "test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, predicted_neighbourhoods, expected_scores, num_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "one_to_ten = [1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "y_predict = np.ndarray((10,),buffer = np.array(one_to_ten))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(len(y_actual), reverse = False), dtype=object)\n",
    "predicted_neighbourhoods = np.array(get_x_neighbourhoods(len(y_predict), reverse = False), dtype=object)\n",
    "expected_scores = [1,1,1,1,1,0]\n",
    "num_hotspots = 9\n",
    "test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, predicted_neighbourhoods, expected_scores, num_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "one_to_ten = [1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "y_predict = np.ndarray((10,),buffer = np.array(one_to_ten))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(len(y_actual), reverse = True), dtype=object)\n",
    "predicted_neighbourhoods = np.array(get_x_neighbourhoods(len(y_predict), reverse = False), dtype=object)\n",
    "expected_scores = [0.8888888888888888,0,0.8888888888888888,0.88889,-0.11111,0.16666666666666666]\n",
    "num_hotspots = 9\n",
    "test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, predicted_neighbourhoods, expected_scores, num_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "one_to_ten = [1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "y_predict = np.ndarray((10,),buffer = np.array(one_to_ten))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(len(y_actual), reverse = False), dtype=object)\n",
    "predicted_neighbourhoods = np.array(get_x_neighbourhoods(len(y_predict), reverse = True), dtype=object)\n",
    "expected_scores = [0.8888888888888888,0,0.8888888888888888,0.88889,-0.11111,0.16666666666666666]\n",
    "num_hotspots = 9\n",
    "test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, predicted_neighbourhoods, expected_scores, num_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "one_to_ten = [1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0]\n",
    "y_actual = np.ndarray((10,), buffer = np.array(ten_to_one))\n",
    "y_predict = np.ndarray((10,), buffer = np.array(one_to_ten))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(len(y_actual), reverse = True), dtype=object)\n",
    "predicted_neighbourhoods = np.array(get_x_neighbourhoods(len(y_predict), reverse = True), dtype=object)\n",
    "expected_scores = [1,1,1,1,1,0]\n",
    "num_hotspots = 9\n",
    "test_calculate_accuracy(y_actual, actual_neighbourhoods, y_predict, predicted_neighbourhoods, expected_scores, num_hotspots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test determine hotspots <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_determine_hotspots(y_actual, actual_neighbourhoods, prediction_neighbourhoods,num_hotspots,expected_result):\n",
    "    result = determine_hotspots(y_actual, actual_neighbourhoods, prediction_neighbourhoods,num_hotspots)[0]\n",
    "    for i in range (0,len(expected_result)):\n",
    "        assert expected_result[i] in result, \"Hotspots not as expected\"\n",
    "    print(\"All tests completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = False), dtype=object)\n",
    "prediction_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = False), dtype=object)\n",
    "expected_result = np.array(get_x_neighbourhoods(10, reverse = False), dtype=object)\n",
    "num_hotspots = 10\n",
    "test_determine_hotspots(y_actual, actual_neighbourhoods, prediction_neighbourhoods,num_hotspots,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = True), dtype=object)\n",
    "prediction_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = False), dtype=object)\n",
    "expected_result = np.array(get_x_neighbourhoods(10, reverse = False), dtype=object)\n",
    "num_hotspots = 10\n",
    "test_determine_hotspots(y_actual, actual_neighbourhoods, prediction_neighbourhoods,num_hotspots,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = False), dtype=object)\n",
    "prediction_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = True), dtype=object)\n",
    "expected_result = np.array(get_x_neighbourhoods(10, reverse = False), dtype=object)\n",
    "num_hotspots = 10\n",
    "test_determine_hotspots(y_actual, actual_neighbourhoods, prediction_neighbourhoods,num_hotspots,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = True), dtype=object)\n",
    "prediction_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = True), dtype=object)\n",
    "expected_result = np.array(get_x_neighbourhoods(10, reverse = False), dtype=object)\n",
    "num_hotspots = 10\n",
    "test_determine_hotspots(y_actual, actual_neighbourhoods, prediction_neighbourhoods,num_hotspots,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = False), dtype=object)\n",
    "prediction_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = False), dtype=object)\n",
    "expected_result = np.array(get_x_neighbourhoods(9, reverse = False), dtype=object)\n",
    "num_hotspots = 9\n",
    "test_determine_hotspots(y_actual, actual_neighbourhoods, prediction_neighbourhoods,num_hotspots,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = True), dtype=object)\n",
    "prediction_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = False), dtype=object)\n",
    "expected_result = np.array(get_x_neighbourhoods(9, reverse = False), dtype=object)\n",
    "num_hotspots = 9\n",
    "test_determine_hotspots(y_actual, actual_neighbourhoods, prediction_neighbourhoods,num_hotspots,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = False), dtype=object)\n",
    "prediction_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = True), dtype=object)\n",
    "expected_result = np.array(['Neighbourhood 9','Neighbourhood 8', 'Neighbourhood 7', 'Neighbourhood 6', 'Neighbourhood 5' ,\n",
    "                   'Neighbourhood 4', 'Neighbourhood 3', 'Neighbourhood 2', 'Neighbourhood 1'], dtype=object)\n",
    "num_hotspots = 9\n",
    "test_determine_hotspots(y_actual, actual_neighbourhoods, prediction_neighbourhoods,num_hotspots,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_to_one = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(ten_to_one))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = True), dtype=object)\n",
    "prediction_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = True), dtype=object)\n",
    "expected_result = np.array(['Neighbourhood 9','Neighbourhood 8', 'Neighbourhood 7', 'Neighbourhood 6', 'Neighbourhood 5' ,\n",
    "                   'Neighbourhood 4', 'Neighbourhood 3', 'Neighbourhood 2', 'Neighbourhood 1'], dtype=object)\n",
    "num_hotspots = 9\n",
    "test_determine_hotspots(y_actual, actual_neighbourhoods, prediction_neighbourhoods,num_hotspots,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_to_zero = [5.0,4.0,3.0,2.0,1.0,1.0,1.0,0.0,0.0,0.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(five_to_zero))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = False), dtype=object)\n",
    "prediction_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = False), dtype=object)\n",
    "expected_result = np.array(get_x_neighbourhoods(5, reverse = False), dtype=object)\n",
    "num_hotspots = 5\n",
    "test_determine_hotspots(y_actual, actual_neighbourhoods, prediction_neighbourhoods,num_hotspots,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_to_zero = [5.0,4.0,3.0,2.0,1.0,1.0,1.0,0.0,0.0,0.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(five_to_zero))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = True), dtype=object)\n",
    "prediction_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = False), dtype=object)\n",
    "expected_result = np.array(get_x_neighbourhoods(5, reverse = False), dtype=object)\n",
    "num_hotspots = 5\n",
    "test_determine_hotspots(y_actual, actual_neighbourhoods, prediction_neighbourhoods,num_hotspots,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_to_zero = [5.0,4.0,3.0,2.0,1.0,1.0,1.0,0.0,0.0,0.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(five_to_zero))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = False), dtype=object)\n",
    "prediction_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = True), dtype=object)\n",
    "expected_result = np.array(['Neighbourhood 9', 'Neighbourhood 8', 'Neighbourhood 7', 'Neighbourhood 6',\n",
    "                            'Neighbourhood 5'], dtype=object)\n",
    "num_hotspots = 5\n",
    "test_determine_hotspots(y_actual, actual_neighbourhoods, prediction_neighbourhoods,num_hotspots,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_to_zero = [5.0,4.0,3.0,2.0,1.0,1.0,1.0,0.0,0.0,0.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(five_to_zero))\n",
    "actual_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = False), dtype=object)\n",
    "prediction_neighbourhoods = np.array(get_x_neighbourhoods(10, reverse = False), dtype=object)\n",
    "expected_result = np.array(get_x_neighbourhoods(5, reverse = True), dtype=object)\n",
    "num_hotspots = 5\n",
    "test_determine_hotspots(y_actual, actual_neighbourhoods, prediction_neighbourhoods,num_hotspots,expected_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test calculate_standard_scores <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_calculate_standard_scores(predicted_hotspots, actual_hotspots, total_predictions,expected):\n",
    "    num_predictions = len(predicted_hotspots)\n",
    "    expected_result = np.ndarray((5,),buffer = np.array(expected))\n",
    "    result = calculate_standard_scores(num_predictions, predicted_hotspots, actual_hotspots, total_predictions)\n",
    "    assert len(expected_result) == len(result), \"Scores not as expected\"\n",
    "    for i in range (0,len(expected_result)):\n",
    "        assert expected_result[i] == result[i], \"Scores not as expected\"\n",
    "    print(\"All tests completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5']\n",
    "actual_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5']\n",
    "total_predictions = 5\n",
    "expected_result = [1.0,1.0,1.0,1.0,1.0]\n",
    "test_calculate_standard_scores(predicted_hotspots, actual_hotspots, total_predictions,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5']\n",
    "actual_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5']\n",
    "total_predictions = 10\n",
    "expected_result = [1.0,1.0,1.0,1.0,1.0]\n",
    "test_calculate_standard_scores(predicted_hotspots, actual_hotspots, total_predictions,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5']\n",
    "actual_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 6']\n",
    "total_predictions = 5\n",
    "expected_result = [0.8,1,0.8,0.8,1.0]\n",
    "test_calculate_standard_scores(predicted_hotspots, actual_hotspots, total_predictions,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5']\n",
    "actual_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 6']\n",
    "total_predictions = 10\n",
    "expected_result = [0.8,0.8,0.8,0.8,0.6]\n",
    "test_calculate_standard_scores(predicted_hotspots, actual_hotspots, total_predictions,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5']\n",
    "actual_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 6']\n",
    "total_predictions = 15\n",
    "expected_result = [0.8,0.9,0.8,0.8,0.7]\n",
    "test_calculate_standard_scores(predicted_hotspots, actual_hotspots, total_predictions,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5']\n",
    "actual_hotspots = ['Neighbourhood 6','Neighbourhood 7','Neighbourhood 8','Neighbourhood 9','Neighbourhood 10']\n",
    "total_predictions = 5\n",
    "expected_result = [0.0,1.0,0.0,0.0,1.0]\n",
    "test_calculate_standard_scores(predicted_hotspots, actual_hotspots, total_predictions,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5']\n",
    "actual_hotspots = ['Neighbourhood 6','Neighbourhood 7','Neighbourhood 8','Neighbourhood 9','Neighbourhood 10']\n",
    "total_predictions = 10\n",
    "expected_result = [0.0,0.0,0.0,0.0,-1.0]\n",
    "test_calculate_standard_scores(predicted_hotspots, actual_hotspots, total_predictions,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5']\n",
    "actual_hotspots = ['Neighbourhood 6','Neighbourhood 7','Neighbourhood 8','Neighbourhood 9','Neighbourhood 10']\n",
    "total_predictions = 15\n",
    "expected_result = [0.0,0.5,0.0,0.0,-0.5]\n",
    "test_calculate_standard_scores(predicted_hotspots, actual_hotspots, total_predictions,expected_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test get_missed_incidents <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_missed_incidents(predicted_hotspots, actual_hotspots, y_actual, y_predict, actual_neighbourhoods,expected_result):\n",
    "    num_predictions = len(predicted_hotspots)\n",
    "    \n",
    "    \n",
    "    actual_result = get_missed_incidents(predicted_hotspots, actual_hotspots, y_actual, y_predict, num_predictions, \n",
    "                                  actual_neighbourhoods)\n",
    "\n",
    "    assert actual_result[0] == expected_result[0], \"Result not as expected.\"\n",
    "    print(\"All tests completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(actuals))\n",
    "predicts = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_predict = np.ndarray((10,),buffer = np.array(predicts))\n",
    "actual_neighbourhoods = np.array(['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5',\n",
    "                                  'Neighbourhood 6','Neighbourhood 7','Neighbourhood 8','Neighbourhood 9','Neighbourhood 10'], \n",
    "                                 dtype=object)\n",
    "predicted_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5']\n",
    "actual_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5']\n",
    "\n",
    "\n",
    "expected_result = [0]\n",
    "test_get_missed_incidents(predicted_hotspots, actual_hotspots, y_actual, y_predict, actual_neighbourhoods,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(actuals))\n",
    "predicts = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_predict = np.ndarray((10,),buffer = np.array(predicts))\n",
    "actual_neighbourhoods = np.array(['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5',\n",
    "                                  'Neighbourhood 6','Neighbourhood 7','Neighbourhood 8','Neighbourhood 9','Neighbourhood 10'], \n",
    "                                 dtype=object)\n",
    "predicted_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 6']\n",
    "actual_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5']\n",
    "actual_hotspot_incidents = 10+9+8+7+6\n",
    "predicted_hotspot_incidents = 10+9+8+7+5\n",
    "missed = actual_hotspot_incidents - predicted_hotspot_incidents\n",
    "expected_result = [missed/actual_hotspot_incidents]\n",
    "\n",
    "test_get_missed_incidents(predicted_hotspots, actual_hotspots, y_actual, y_predict, actual_neighbourhoods,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(actuals))\n",
    "predicts = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_predict = np.ndarray((10,),buffer = np.array(predicts))\n",
    "actual_neighbourhoods = np.array(['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5',\n",
    "                                  'Neighbourhood 6','Neighbourhood 7','Neighbourhood 8','Neighbourhood 9','Neighbourhood 10'], \n",
    "                                 dtype=object)\n",
    "predicted_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 7']\n",
    "actual_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5']\n",
    "actual_hotspot_incidents = 10+9+8+7+6\n",
    "predicted_hotspot_incidents = 10+9+8+7+4\n",
    "missed = actual_hotspot_incidents - predicted_hotspot_incidents\n",
    "expected_result = [missed/actual_hotspot_incidents]\n",
    "\n",
    "test_get_missed_incidents(predicted_hotspots, actual_hotspots, y_actual, y_predict, actual_neighbourhoods,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "actuals = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(actuals))\n",
    "predicts = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_predict = np.ndarray((10,),buffer = np.array(predicts))\n",
    "actual_neighbourhoods = np.array(['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5',\n",
    "                                  'Neighbourhood 6','Neighbourhood 7','Neighbourhood 8','Neighbourhood 9','Neighbourhood 10'], \n",
    "                                 dtype=object)\n",
    "predicted_hotspots = ['Neighbourhood 6','Neighbourhood 7','Neighbourhood 8','Neighbourhood 9','Neighbourhood 10']\n",
    "actual_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5']\n",
    "actual_hotspot_incidents = 10+9+8+7+6\n",
    "predicted_hotspot_incidents = 1+2+3+4+5\n",
    "missed = actual_hotspot_incidents - predicted_hotspot_incidents\n",
    "expected_result = [missed/actual_hotspot_incidents]\n",
    "\n",
    "test_get_missed_incidents(predicted_hotspots, actual_hotspots, y_actual, y_predict, actual_neighbourhoods,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(actuals))\n",
    "predicts = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_predict = np.ndarray((10,),buffer = np.array(predicts))\n",
    "actual_neighbourhoods = np.array(['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5',\n",
    "                                  'Neighbourhood 6','Neighbourhood 7','Neighbourhood 8','Neighbourhood 9','Neighbourhood 10'], \n",
    "                                 dtype=object)\n",
    "predicted_hotspots = ['Neighbourhood 10','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5']\n",
    "actual_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5']\n",
    "actual_hotspot_incidents = 10+9+8+7+6\n",
    "predicted_hotspot_incidents = 1+9+8+7+6\n",
    "missed = actual_hotspot_incidents - predicted_hotspot_incidents\n",
    "expected_result = [missed/actual_hotspot_incidents]\n",
    "\n",
    "test_get_missed_incidents(predicted_hotspots, actual_hotspots, y_actual, y_predict, actual_neighbourhoods,expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = [10.0,9.0,8.0,7.0,6.0,0.0,0.0,0.0,0.0,0.0]\n",
    "y_actual = np.ndarray((10,),buffer = np.array(actuals))\n",
    "predicts = [10.0,9.0,8.0,7.0,6.0,5.0,4.0,3.0,2.0,1.0]\n",
    "y_predict = np.ndarray((10,),buffer = np.array(predicts))\n",
    "actual_neighbourhoods = np.array(['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5',\n",
    "                                  'Neighbourhood 6','Neighbourhood 7','Neighbourhood 8','Neighbourhood 9','Neighbourhood 10'], \n",
    "                                 dtype=object)\n",
    "predicted_hotspots = ['Neighbourhood 6','Neighbourhood 7','Neighbourhood 8','Neighbourhood 9','Neighbourhood 10']\n",
    "actual_hotspots = ['Neighbourhood 1','Neighbourhood 2','Neighbourhood 3','Neighbourhood 4','Neighbourhood 5']\n",
    "actual_hotspot_incidents = 10+9+8+7+6\n",
    "predicted_hotspot_incidents = 0+0+0+0+0\n",
    "missed = actual_hotspot_incidents - predicted_hotspot_incidents\n",
    "expected_result = [missed/actual_hotspot_incidents]\n",
    "\n",
    "test_get_missed_incidents(predicted_hotspots, actual_hotspots, y_actual, y_predict, actual_neighbourhoods,expected_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test score_algorithm <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_score_algorithm(model_key, feature_key, num_hotspots, first_date, last_date, df, expected):\n",
    "    result = score_algorithm(model_key, feature_key, num_hotspots, first_date, last_date, df)\n",
    "    for key in expected:\n",
    "        assert key in result, key + \" key expected but not found.\"\n",
    "        expected_key_sore = expected[key]\n",
    "        actual_key_score = result[key]\n",
    "        assert len(expected_key_sore) == len(actual_key_score), \"Scores for \" + key + \" key are not of expected length.\"\n",
    "        for i in range (0, len(expected_key_sore)):\n",
    "            assert expected_key_sore[i] == actual_key_score[i], \"Scores for \" + key + \" not as expected\"\n",
    "    print(\"All tests completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"tuning_test_data\"\n",
    "df = open_file(file_name)\n",
    "first_date = datetime.strptime(df['Date'].iloc[0], '%d/%m/%Y')\n",
    "last_date = datetime.strptime(df['Date'].iloc[len(df) - 1], '%d/%m/%Y')\n",
    "model_key = \"tuning_template_test_model\"\n",
    "feature_key = \"arbitrary_name\"\n",
    "FEATURES[feature_key] = FEATURES[F_REGRESSION_NAME]\n",
    "MODEL_FILE_TAGS[model_key] = model_key\n",
    "FEATURE_FILE_TAGS[feature_key] = feature_key\n",
    "num_hotspots = 5\n",
    "expected = {'Sensitivity': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], \n",
    "            'Specificity': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], \n",
    "            'Precision': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], \n",
    "            'F1': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], \n",
    "            'MCC': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], \n",
    "            'Lowest Misclassification Severity': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}\n",
    "test_score_algorithm(model_key, feature_key, num_hotspots, first_date, last_date, df, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test calculate_averages <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_calulate_averages(scores, expected_averages):\n",
    "    actual_averages = calculate_averages(scores)\n",
    "    for key in expected_averages:\n",
    "        assert key in actual_averages, key + \" key expected but not found.\"\n",
    "        expected_key_sore = expected_averages[key]\n",
    "        actual_key_score = actual_averages[key]\n",
    "        assert expected_key_sore == actual_key_score, \"Scores for \" + key + \" not as expected\"\n",
    "    print(\"All tests completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {'Sensitivity': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], \n",
    "            'Specificity': [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0], \n",
    "            'Precision': [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0], \n",
    "            'F1': [4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0], \n",
    "            'MCC': [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0], \n",
    "            'Lowest Misclassification Severity': [6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0]}\n",
    "expected_averages = {'Sensitivity': 1.0, \n",
    "                     'Specificity': 2.0, \n",
    "                     'Precision': 3.0, \n",
    "                     'F1': 4.0, \n",
    "                     'MCC': 5.0, \n",
    "                     'Lowest Misclassification Severity': 6.0}\n",
    "test_calulate_averages(scores, expected_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {'Sensitivity': [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0], \n",
    "            'Specificity': [1, 2, 3], \n",
    "            'Precision': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0], \n",
    "            'F1': [100000000, 10000000, 1000000], \n",
    "            'MCC': [0.5,1.5], \n",
    "            'Lowest Misclassification Severity': [1,1.5,2,2.5,3,3.5]}\n",
    "expected_averages = {'Sensitivity': 4.0, \n",
    "                     'Specificity': 2, \n",
    "                     'Precision': 0.55, \n",
    "                     'F1': 37000000, \n",
    "                     'MCC': 1.0, \n",
    "                     'Lowest Misclassification Severity': 2.25}\n",
    "test_calulate_averages(scores, expected_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {'Sensitivity': [0], \n",
    "            'Specificity': [1], \n",
    "            'Precision': [0,0,0,0], \n",
    "            'F1': [-1,0,1], \n",
    "            'MCC': [-1,0,4], \n",
    "            'Lowest Misclassification Severity': []}\n",
    "expected_averages = {'Sensitivity': 0, \n",
    "                     'Specificity': 1, \n",
    "                     'Precision': 0, \n",
    "                     'F1': 0, \n",
    "                     'MCC': 1, \n",
    "                     'Lowest Misclassification Severity': 0}\n",
    "test_calulate_averages(scores, expected_averages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test get_results <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_results(num_hotspots, file_name, expected_scores, expected_names):\n",
    "    df = open_file(file_name)\n",
    "    first_date = datetime.strptime(df['Date'].iloc[0], '%d/%m/%Y')\n",
    "    last_date = datetime.strptime(df['Date'].iloc[len(df) - 1], '%d/%m/%Y')\n",
    "    all_scores, algorithm_names = get_results(num_hotspots, first_date, last_date, df)\n",
    "    assert len(expected_names) == len(algorithm_names), \"Algorithm names length not as expected\"\n",
    "    for i in range(0, len(expected_names)):\n",
    "        assert expected_names[i] == algorithm_names[i], \"Algorithm names not as expected\"\n",
    "    for key in all_scores:\n",
    "        assert key in SCORE_KEYS, \"All scores keys not as expected\"\n",
    "        if len(expected_scores[key]) > 0:\n",
    "            assert len(all_scores[key]) == len(expected_scores[key]), \"Score length not as expected\"\n",
    "            for i in range(0,len(expected_scores[key])):\n",
    "                assert expected_scores[key][i] == all_scores[key][i], \"Score for \" + key + \" not as expected\"\n",
    "    print(\"All tests completed successfully\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"tuning_test_data\"\n",
    "model_key = \"tuning_template_test_model\"\n",
    "feature_key = \"arbitrary_name\"\n",
    "MODEL_FILE_TAGS[model_key] = model_key\n",
    "FEATURE_FILE_TAGS[feature_key] = feature_key\n",
    "MODELS = {model_key : model_key}\n",
    "FEATURES = {feature_key : ['Reports 1 day ago', 'Reports 2 days ago', 'Reports 3 days ago',\n",
    "                       'Reports 4 days ago', 'Reports 5 days ago', 'Reports 6 days ago',\n",
    "                      'Reports 7 days ago','Reports 14 days ago','Reports 30 days ago','Reports 365 days ago']}\n",
    "num_hotspots = 5\n",
    "expected_scores = {'Sensitivity': [1.0], \n",
    "                   'Specificity': [1.0], \n",
    "                   'Precision': [1.0], \n",
    "                   'F1': [1.0], \n",
    "                   'MCC': [1.0], \n",
    "                   'Lowest Misclassification Severity': [0.0]}\n",
    "expected_names = ['tuning_template_test_model\\narbitrary_name']\n",
    "test_get_results(num_hotspots, file_name, expected_scores, expected_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FEATURES = {\n",
    "    F_REGRESSION_NAME : ['Reports 1 day ago', 'Reports 2 days ago', 'Reports 3 days ago',\n",
    "                       'Reports 4 days ago', 'Reports 5 days ago', 'Reports 6 days ago',\n",
    "                      'Reports 7 days ago','Reports 14 days ago','Reports 30 days ago','Reports 365 days ago'],\n",
    "    CHI2_NAME : ['South of Market', 'Mission', 'Tenderloin', 'Number of businesses', \n",
    "               'Downtown / Union Square', 'Civic Center', 'Reports 365 days ago',\n",
    "               'Reports 1 day ago','Reports 2 days ago','Reports 14 days ago'],\n",
    "    ADABOOST_NAME : ['Reports 365 days ago', 'Reports 1 day ago', 'Reports 14 days ago', 'Reports 3 days ago', \n",
    "               'Reports 2 days ago', 'Reports 7 days ago', 'Number of businesses',\n",
    "               'Reports 4 days ago','Reports 5 days ago','Closures 365 days ago'],\n",
    "    EQUAL_DATA_NAME : ['Number of businesses', 'Last 28 days closures', 'Last 7 days openings',\n",
    "                          'Last 14 days closures', 'Last 7 days closures','Reports 1 day ago',\n",
    "                      'Reports 2 days ago', 'Reports 4 days ago', 'Reports 30 days ago', 'Reports 7 days ago'],\n",
    "    ALL_BUS_NAME : ['Number of businesses', 'Last 28 days closures', 'Last 7 days openings',\n",
    "                          'Last 14 days closures', 'Last 7 days closures','Number of openings',\n",
    "                   'Openings 4 days ago','Openings 1 day ago', 'Openings 7 days ago', 'Openings 2 days ago']\n",
    "    }\n",
    "MODELS = [ANN_NAME,\n",
    "          DECISION_TREE_NAME,\n",
    "          ELASTIC_NET_NAME,\n",
    "          LASSO_NAME,\n",
    "          LINERAR_REGRESSION_NAME,\n",
    "          RANDOM_FOREST_NAME,\n",
    "          RIDGE_REGRESSION_NAME,\n",
    "          SVM_NAME]\n",
    "file_name = \"tuning_test_data\"\n",
    "num_hotspots = 5\n",
    "expected_scores = {'Sensitivity': [], \n",
    "                   'Specificity': [], \n",
    "                   'Precision': [], \n",
    "                   'F1': [], \n",
    "                   'MCC': [], \n",
    "                   'Lowest Misclassification Severity': []}\n",
    "expected_names = ['Multi-Layer\\nPerceptron\\nF-Regression', 'Multi-Layer\\nPerceptron\\nChi-Squared', \n",
    "                  'Multi-Layer\\nPerceptron\\nAdaBoost', 'Multi-Layer\\nPerceptron\\nEqual Selection', \n",
    "                  'Multi-Layer\\nPerceptron\\nAll Business', 'Decision Tree\\nF-Regression', \n",
    "                  'Decision Tree\\nChi-Squared', 'Decision Tree\\nAdaBoost', 'Decision Tree\\nEqual Selection', \n",
    "                  'Decision Tree\\nAll Business', 'Elastic Net\\nF-Regression', 'Elastic Net\\nChi-Squared', \n",
    "                  'Elastic Net\\nAdaBoost', 'Elastic Net\\nEqual Selection', 'Elastic Net\\nAll Business', \n",
    "                  'Lasso\\nF-Regression', 'Lasso\\nChi-Squared', 'Lasso\\nAdaBoost', 'Lasso\\nEqual Selection', \n",
    "                  'Lasso\\nAll Business', 'Linear \\nRegression\\nF-Regression', 'Linear \\nRegression\\nChi-Squared', \n",
    "                  'Linear \\nRegression\\nAdaBoost', 'Linear \\nRegression\\nEqual Selection', \n",
    "                  'Linear \\nRegression\\nAll Business', 'Random \\nForest\\nF-Regression', 'Random \\nForest\\nChi-Squared', \n",
    "                  'Random \\nForest\\nAdaBoost', 'Random \\nForest\\nEqual Selection', 'Random \\nForest\\nAll Business', \n",
    "                  'Ridge \\nRegression\\nF-Regression', 'Ridge \\nRegression\\nChi-Squared', 'Ridge \\nRegression\\nAdaBoost', \n",
    "                  'Ridge \\nRegression\\nEqual Selection', 'Ridge \\nRegression\\nAll Business', 'SVM\\nF-Regression', \n",
    "                  'SVM\\nChi-Squared', 'SVM\\nAdaBoost', 'SVM\\nEqual Selection', 'SVM\\nAll Business']\n",
    "test_get_results(num_hotspots, file_name, expected_scores, expected_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test get_best_x_scores <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_lists():\n",
    "    algorithms = ['multi_layer_perceptron',\n",
    "                  'decision_tree',\n",
    "                  'elastic_net',\n",
    "                  'lasso',\n",
    "                  'linear_regression',\n",
    "                  'random_forest',\n",
    "                  'ridge_regression',\n",
    "                  'svm']\n",
    "    algorithm_display_names = ['Multi\\nLayer\\nPerceptron',\n",
    "                               'Decision\\nTree',\n",
    "                               'Elastic Net',\n",
    "                               'Lasso',\n",
    "                               'Linear\\nRegression',\n",
    "                               'Random\\nForest',\n",
    "                               'Ridge\\nRegression',\n",
    "                               'SVM']\n",
    "    feature_select_display_names = ['F Regression',\n",
    "                               'Chi2',\n",
    "                               'Adaboost',\n",
    "                               'Equal crime\\nand business',\n",
    "                               'All Business']\n",
    "    highest_is_best = {\"r2\":True,\n",
    "                  \"mse\":False,\n",
    "                  \"mae\":False}\n",
    "    score_metrics = [\"r2\",\"mse\",\"mae\"]\n",
    "    return algorithms, algorithm_display_names, feature_select_display_names, highest_is_best, score_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best_x_scores(bar, expected_heights, x_label, y_label, expected_y, x_ticks, expected_ticks, expected_title, \n",
    "                       title, expected_x):\n",
    "    \n",
    "    assert title.get_text() == expected_title, \"Title not as expected.\"\n",
    "    rectangles = bar.get_children()\n",
    "    for i in range(len(rectangles)):\n",
    "        assert rectangles[i].get_height() == expected_heights[i],\"Height of a rectangle not as expected.\"\n",
    "    assert x_label.get_text() == \"Algorithm\", \"x label not as expected.\"\n",
    "    assert y_label.get_text() == expected_y, \"y label not as expected.\"\n",
    "    sub_headings = x_ticks[1]\n",
    "    assert sub_headings, \"Sub-headings not as expected\"\n",
    "    assert len(sub_headings) == expected_x and len(rectangles) == expected_x, \"Number of columns not as expected.\"\n",
    "    for i in range(0,len(sub_headings)):\n",
    "        assert sub_headings[i].get_text() == expected_ticks[i], \"Sub-headings not as expected\"\n",
    "    print(\"All bar chart tests completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_r2_rectangle_heights = [1]\n",
    "algorithms, algorithm_display_names, feature_select_display_names, highest_is_best, score_metrics = setup_lists()\n",
    "scores = {'Sensitivity': [1]}\n",
    "metric = 'Sensitivity'\n",
    "x = 10\n",
    "num_hotspots = 5\n",
    "expected_y = 'Sensitivity score'\n",
    "expected_x = 1\n",
    "expected_title = \"Top 1 Sensitivity scores\\nwhen predicting the top 5 Crime Hotspots\"\n",
    "expected_ticks = ['Multi\\nLayer\\nPerceptron\\nusing\\nF Regression\\ndataset']\n",
    "bar, ticks, title, x_label, y_label = best_x_scores(metric,x,algorithms, scores, num_hotspots)\n",
    "test_best_x_scores(bar, expected_r2_rectangle_heights, x_label, y_label, expected_y, ticks, expected_ticks, expected_title, \n",
    "               title, expected_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_r2_rectangle_heights = [2,1]\n",
    "algorithms, algorithm_display_names, feature_select_display_names, highest_is_best, score_metrics = setup_lists()\n",
    "scores = {'Sensitivity': [1,2],\n",
    "         'Specificity': [],\n",
    "         'Precision':[],\n",
    "         'F1':[],\n",
    "         'MCC':[],\n",
    "         'Lowest Misclassification Severity':[]}\n",
    "metric = 'Sensitivity'\n",
    "x = 10\n",
    "num_hotspots = 5\n",
    "expected_y = 'Sensitivity score'\n",
    "expected_x = 2\n",
    "expected_title = \"Top 2 Sensitivity scores\\nwhen predicting the top 5 Crime Hotspots\"\n",
    "expected_ticks = ['Multi\\nLayer\\nPerceptron\\nusing\\nChi2\\ndataset',\n",
    "                  'Multi\\nLayer\\nPerceptron\\nusing\\nF Regression\\ndataset']\n",
    "bar, ticks, title, x_label, y_label = best_x_scores(metric,x,algorithms, scores, num_hotspots)\n",
    "test_best_x_scores(bar, expected_r2_rectangle_heights, x_label, y_label, expected_y, ticks, expected_ticks, expected_title, \n",
    "               title, expected_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_r2_rectangle_heights = [3,2,1]\n",
    "algorithms, algorithm_display_names, feature_select_display_names, highest_is_best, score_metrics = setup_lists()\n",
    "scores = {'Sensitivity': [1,2],\n",
    "         'Specificity': [2,3,1],\n",
    "         'Precision':[],\n",
    "         'F1':[],\n",
    "         'MCC':[],\n",
    "         'Lowest Misclassification Severity':[]}\n",
    "metric = 'Specificity'\n",
    "x = 10\n",
    "num_hotspots = 5\n",
    "expected_y = 'Specificity score'\n",
    "expected_x = 3\n",
    "expected_title = \"Top 3 Specificity scores\\nwhen predicting the top 5 Crime Hotspots\"\n",
    "expected_ticks = ['Multi\\nLayer\\nPerceptron\\nusing\\nChi2\\ndataset',\n",
    "                  'Multi\\nLayer\\nPerceptron\\nusing\\nF Regression\\ndataset',\n",
    "                 'Multi\\nLayer\\nPerceptron\\nusing\\nAdaboost\\ndataset']\n",
    "bar, ticks, title, x_label, y_label = best_x_scores(metric,x,algorithms, scores, num_hotspots)\n",
    "test_best_x_scores(bar, expected_r2_rectangle_heights, x_label, y_label, expected_y, ticks, expected_ticks, expected_title, \n",
    "               title, expected_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_r2_rectangle_heights = [6,5,4,3,2]\n",
    "algorithms, algorithm_display_names, feature_select_display_names, highest_is_best, score_metrics = setup_lists()\n",
    "scores = {'Sensitivity': [1,2],\n",
    "         'Specificity': [2,3,1],\n",
    "         'Precision':[2,4,6,1,3,5],\n",
    "         'F1':[],\n",
    "         'MCC':[],\n",
    "         'Lowest Misclassification Severity':[]}\n",
    "metric = 'Precision'\n",
    "x = 5\n",
    "num_hotspots = 5\n",
    "expected_y = 'Precision score'\n",
    "expected_x = 5\n",
    "expected_title = \"Top 5 Precision scores\\nwhen predicting the top 5 Crime Hotspots\"\n",
    "expected_ticks = ['Multi\\nLayer\\nPerceptron\\nusing\\nAdaboost\\ndataset',\n",
    "                  'Decision\\nTree\\nusing\\nF Regression\\ndataset',\n",
    "                  'Multi\\nLayer\\nPerceptron\\nusing\\nChi2\\ndataset',\n",
    "                  'Multi\\nLayer\\nPerceptron\\nusing\\nAll Business\\ndataset',\n",
    "                  'Multi\\nLayer\\nPerceptron\\nusing\\nF Regression\\ndataset',\n",
    "                 ]\n",
    "bar, ticks, title, x_label, y_label = best_x_scores(metric,x,algorithms, scores, num_hotspots)\n",
    "test_best_x_scores(bar, expected_r2_rectangle_heights, x_label, y_label, expected_y, ticks, expected_ticks, expected_title, \n",
    "               title, expected_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_r2_rectangle_heights = [15,14,13,12,11,10]\n",
    "algorithms, algorithm_display_names, feature_select_display_names, highest_is_best, score_metrics = setup_lists()\n",
    "scores = {'Sensitivity': [1,2],\n",
    "         'Specificity': [2,3,1],\n",
    "         'Precision':[2,4,6,1,3,5],\n",
    "         'F1':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],\n",
    "         'MCC':[],\n",
    "         'Lowest Misclassification Severity':[]}\n",
    "metric = 'F1'\n",
    "x = 6\n",
    "num_hotspots = 5\n",
    "expected_y = 'F1 score'\n",
    "expected_x = 6\n",
    "expected_title = \"Top 6 F1 scores\\nwhen predicting the top 5 Crime Hotspots\"\n",
    "expected_ticks = ['Elastic Net\\nusing\\nAll Business\\ndataset',\n",
    "                  'Elastic Net\\nusing\\nEqual crime\\nand business\\ndataset',\n",
    "                  'Elastic Net\\nusing\\nAdaboost\\ndataset',\n",
    "                  'Elastic Net\\nusing\\nChi2\\ndataset',\n",
    "                  'Elastic Net\\nusing\\nF Regression\\ndataset',\n",
    "                  'Decision\\nTree\\nusing\\nAll Business\\ndataset'\n",
    "                 ]\n",
    "bar, ticks, title, x_label, y_label = best_x_scores(metric,x,algorithms, scores, num_hotspots)\n",
    "test_best_x_scores(bar, expected_r2_rectangle_heights, x_label, y_label, expected_y, ticks, expected_ticks, expected_title, \n",
    "               title, expected_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_r2_rectangle_heights = [3,2,1]\n",
    "algorithms, algorithm_display_names, feature_select_display_names, highest_is_best, score_metrics = setup_lists()\n",
    "scores = {'Sensitivity': [1,2],\n",
    "         'Specificity': [1],\n",
    "         'Precision':[1],\n",
    "         'F1':[1],\n",
    "         'MCC':[2,3,1],\n",
    "         'Lowest Misclassification Severity':[]}\n",
    "metric = 'MCC'\n",
    "x = 10\n",
    "num_hotspots = 5\n",
    "expected_y = 'MCC score'\n",
    "expected_x = 3\n",
    "expected_title = \"Top 3 MCC scores\\nwhen predicting the top 5 Crime Hotspots\"\n",
    "expected_ticks = ['Multi\\nLayer\\nPerceptron\\nusing\\nChi2\\ndataset',\n",
    "                  'Multi\\nLayer\\nPerceptron\\nusing\\nF Regression\\ndataset',\n",
    "                 'Multi\\nLayer\\nPerceptron\\nusing\\nAdaboost\\ndataset']\n",
    "bar, ticks, title, x_label, y_label = best_x_scores(metric,x,algorithms, scores, num_hotspots)\n",
    "test_best_x_scores(bar, expected_r2_rectangle_heights, x_label, y_label, expected_y, ticks, expected_ticks, expected_title, \n",
    "               title, expected_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_r2_rectangle_heights = [1,2,3]\n",
    "algorithms, algorithm_display_names, feature_select_display_names, highest_is_best, score_metrics = setup_lists()\n",
    "scores = {'Sensitivity': [1,2],\n",
    "         'Specificity': [1],\n",
    "         'Precision':[1],\n",
    "         'F1':[1],\n",
    "         'MCC':[1],\n",
    "         'Lowest Misclassification Severity':[2,1,3]}\n",
    "metric = 'Lowest Misclassification Severity'\n",
    "x = 10\n",
    "num_hotspots = 5\n",
    "expected_y = 'Lowest Misclassification Severity score'\n",
    "expected_x = 3\n",
    "expected_title = \"Top 3 Lowest Misclassification Severity scores\\nwhen predicting the top 5 Crime Hotspots\"\n",
    "expected_ticks = ['Multi\\nLayer\\nPerceptron\\nusing\\nChi2\\ndataset',\n",
    "                  'Multi\\nLayer\\nPerceptron\\nusing\\nF Regression\\ndataset',\n",
    "                 'Multi\\nLayer\\nPerceptron\\nusing\\nAdaboost\\ndataset']\n",
    "bar, ticks, title, x_label, y_label = best_x_scores(metric,x,algorithms, scores, num_hotspots)\n",
    "test_best_x_scores(bar, expected_r2_rectangle_heights, x_label, y_label, expected_y, ticks, expected_ticks, expected_title, \n",
    "               title, expected_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test get_labels <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_labels():\n",
    "    expected_labels = ['Multi\\nLayer\\nPerceptron\\nusing\\nF Regression\\ndataset', \n",
    "                       'Multi\\nLayer\\nPerceptron\\nusing\\nChi2\\ndataset', \n",
    "                       'Multi\\nLayer\\nPerceptron\\nusing\\nAdaboost\\ndataset', \n",
    "                       'Multi\\nLayer\\nPerceptron\\nusing\\nEqual crime\\nand business\\ndataset',\n",
    "                       'Multi\\nLayer\\nPerceptron\\nusing\\nAll Business\\ndataset', \n",
    "                       'Decision\\nTree\\nusing\\nF Regression\\ndataset', \n",
    "                       'Decision\\nTree\\nusing\\nChi2\\ndataset', \n",
    "                       'Decision\\nTree\\nusing\\nAdaboost\\ndataset', \n",
    "                       'Decision\\nTree\\nusing\\nEqual crime\\nand business\\ndataset',\n",
    "                       'Decision\\nTree\\nusing\\nAll Business\\ndataset', \n",
    "                       'Elastic Net\\nusing\\nF Regression\\ndataset', \n",
    "                       'Elastic Net\\nusing\\nChi2\\ndataset', \n",
    "                       'Elastic Net\\nusing\\nAdaboost\\ndataset', \n",
    "                       'Elastic Net\\nusing\\nEqual crime\\nand business\\ndataset', \n",
    "                       'Elastic Net\\nusing\\nAll Business\\ndataset', \n",
    "                       'Lasso\\nusing\\nF Regression\\ndataset', \n",
    "                       'Lasso\\nusing\\nChi2\\ndataset', \n",
    "                       'Lasso\\nusing\\nAdaboost\\ndataset', \n",
    "                       'Lasso\\nusing\\nEqual crime\\nand business\\ndataset', \n",
    "                       'Lasso\\nusing\\nAll Business\\ndataset', \n",
    "                       'Linear\\nRegression\\nusing\\nF Regression\\ndataset', \n",
    "                       'Linear\\nRegression\\nusing\\nChi2\\ndataset', \n",
    "                       'Linear\\nRegression\\nusing\\nAdaboost\\ndataset', \n",
    "                       'Linear\\nRegression\\nusing\\nEqual crime\\nand business\\ndataset', \n",
    "                       'Linear\\nRegression\\nusing\\nAll Business\\ndataset', \n",
    "                       'Random\\nForest\\nusing\\nF Regression\\ndataset', \n",
    "                       'Random\\nForest\\nusing\\nChi2\\ndataset', \n",
    "                       'Random\\nForest\\nusing\\nAdaboost\\ndataset', \n",
    "                       'Random\\nForest\\nusing\\nEqual crime\\nand business\\ndataset', \n",
    "                       'Random\\nForest\\nusing\\nAll Business\\ndataset', \n",
    "                       'Ridge\\nRegression\\nusing\\nF Regression\\ndataset', \n",
    "                       'Ridge\\nRegression\\nusing\\nChi2\\ndataset', \n",
    "                       'Ridge\\nRegression\\nusing\\nAdaboost\\ndataset', \n",
    "                       'Ridge\\nRegression\\nusing\\nEqual crime\\nand business\\ndataset', \n",
    "                       'Ridge\\nRegression\\nusing\\nAll Business\\ndataset', \n",
    "                       'SVM\\nusing\\nF Regression\\ndataset', \n",
    "                       'SVM\\nusing\\nChi2\\ndataset', \n",
    "                       'SVM\\nusing\\nAdaboost\\ndataset', \n",
    "                       'SVM\\nusing\\nEqual crime\\nand business\\ndataset', \n",
    "                       'SVM\\nusing\\nAll Business\\ndataset']\n",
    "    actual_labels = get_labels()\n",
    "    assert expected_labels == actual_labels, \"Labels not as expected.\"\n",
    "    print(\"All tests completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Selection Methods\",\"rb\")\n",
    "sel_methods = np.load(file)\n",
    "test_get_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test validate_x <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_validate_x(num_scores, x, expected_x):\n",
    "    x = validate_x(num_scores, x)\n",
    "    assert x == expected_x, \"x not as expected; x is \" + str(x) + \", but expected \" + str(expected_x) + \". i: \" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_scores = [-1,-1,-1,-1,-1,-1,-1, \n",
    "              0,0,0,0,0,0,0,        \n",
    "              1,1,1,1,1,1,1,        \n",
    "              2,2,2,2,2,2,2,        \n",
    "              14,14,14,14,14,14,14, \n",
    "              15,15,15,15,15,15,15, \n",
    "              16,16,16,16,16,16,16] \n",
    "x = [-1,0,1,2,14,15,16,  \n",
    "     -1,0,1,2,14,15,16,  \n",
    "     -1,0,1,2,14,15,16,  \n",
    "     -1,0,1,2,14,15,16,  \n",
    "     -1,0,1,2,14,15,16,  \n",
    "     -1,0,1,2,14,15,16,  \n",
    "     -1,0,1,2,14,15,16,]\n",
    "expected = [0,0,0,0,0,0,0,\n",
    "           0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,\n",
    "           2,2,1,2,2,2,2,\n",
    "           14,14,1,2,14,14,14,\n",
    "           15,15,1,2,14,15,15,\n",
    "           15,15,1,2,14,15,16]\n",
    "for i in range(0, len(num_scores)):\n",
    "    test_validate_x(num_scores[i],x[i],expected[i])\n",
    "print(\"All tests completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test evaluate_models <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluate_models(num_hotspots_range, top_x, file_name, expected_cols_in_bars, expected_scores,expected_num_bars,\n",
    "                        expected_x_ticks, expected_titles,expected_x_label,expected_y_labels):\n",
    "    df = open_file(file_name)\n",
    "    bars = evaluate_models(num_hotspots_range, top_x, df)\n",
    "    assert len(bars) == expected_num_bars, \"Different number of bars than expected.\"\n",
    "    for i in range(0,len(bars)):\n",
    "        rectangles = bars[i][0].get_children()\n",
    "        assert len(rectangles) == expected_cols_in_bars, \"Number of columns not as expected.\"\n",
    "        if len(expected_scores) > 0:\n",
    "            for j in range(0,len(rectangles)):\n",
    "                rectangle = rectangles[j]\n",
    "                assert rectangle.get_height() == expected_scores[i],\"Height of a rectangle not as expected.\"\n",
    "        x_tick_text = bars[i][1][0][0].get_children()[3].get_text()\n",
    "        assert x_tick_text == expected_x_ticks[i], \"X ticks not as expected\"\n",
    "        title = bars[i][2].get_text()\n",
    "        assert title == expected_titles[i], \"Titles not as expected\"\n",
    "        x_label = bars[i][3].get_text()\n",
    "        assert x_label == expected_x_label, \"x label not as expected\"\n",
    "        y_label = bars[i][4].get_text()\n",
    "        assert y_label == expected_y_labels[i], \"y label not as expected\"\n",
    "    print(\"All tests completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file_name = \"tuning_test_data\"\n",
    "model_key = \"tuning_template_test_model\"\n",
    "feature_key = \"arbitrary_name\"\n",
    "MODEL_FILE_TAGS[model_key] = model_key\n",
    "FEATURE_FILE_TAGS[feature_key] = feature_key\n",
    "MODELS = {model_key : model_key}\n",
    "FEATURES = {feature_key : ['Reports 1 day ago', 'Reports 2 days ago', 'Reports 3 days ago',\n",
    "                       'Reports 4 days ago', 'Reports 5 days ago', 'Reports 6 days ago',\n",
    "                      'Reports 7 days ago','Reports 14 days ago','Reports 30 days ago','Reports 365 days ago']}\n",
    "num_hotspots = 5\n",
    "num_hotspots_range = [5]\n",
    "top_x = 5\n",
    "expected_cols_in_bars = 1\n",
    "expected_num_bars = 6\n",
    "expected_scores = [1,1,1,1,1,0]\n",
    "expected_x_ticks = ['Multi\\nLayer\\nPerceptron\\nusing\\nF Regression\\ndataset',\n",
    "                   'Multi\\nLayer\\nPerceptron\\nusing\\nF Regression\\ndataset',\n",
    "                   'Multi\\nLayer\\nPerceptron\\nusing\\nF Regression\\ndataset',\n",
    "                   'Multi\\nLayer\\nPerceptron\\nusing\\nF Regression\\ndataset',\n",
    "                   'Multi\\nLayer\\nPerceptron\\nusing\\nF Regression\\ndataset',\n",
    "                   'Multi\\nLayer\\nPerceptron\\nusing\\nF Regression\\ndataset']\n",
    "expected_titles = ['Top 1 Sensitivity scores\\nwhen predicting the top 5 Crime Hotspots',\n",
    "                  'Top 1 Specificity scores\\nwhen predicting the top 5 Crime Hotspots',\n",
    "                  'Top 1 Precision scores\\nwhen predicting the top 5 Crime Hotspots',\n",
    "                  'Top 1 F1 scores\\nwhen predicting the top 5 Crime Hotspots',\n",
    "                  'Top 1 MCC scores\\nwhen predicting the top 5 Crime Hotspots',\n",
    "                  'Top 1 Lowest Misclassification Severity scores\\nwhen predicting the top 5 Crime Hotspots']\n",
    "expected_x_label = \"Algorithm\"\n",
    "expected_y_labels = ['Sensitivity score', 'Specificity score', 'Precision score', 'F1 score',\n",
    "                    'MCC score', 'Lowest Misclassification Severity score']\n",
    "test_evaluate_models(num_hotspots_range, top_x, file_name, expected_cols_in_bars, expected_scores,expected_num_bars,\n",
    "                    expected_x_ticks,expected_titles,expected_x_label, expected_y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [ANN_NAME,\n",
    "          DECISION_TREE_NAME,\n",
    "          ELASTIC_NET_NAME,\n",
    "          LASSO_NAME,\n",
    "          LINERAR_REGRESSION_NAME,\n",
    "          RANDOM_FOREST_NAME,\n",
    "          RIDGE_REGRESSION_NAME,\n",
    "          SVM_NAME]\n",
    "FEATURES = {\n",
    "    F_REGRESSION_NAME : ['Reports 1 day ago', 'Reports 2 days ago', 'Reports 3 days ago',\n",
    "                       'Reports 4 days ago', 'Reports 5 days ago', 'Reports 6 days ago',\n",
    "                      'Reports 7 days ago','Reports 14 days ago','Reports 30 days ago','Reports 365 days ago'],\n",
    "    CHI2_NAME : ['South of Market', 'Mission', 'Tenderloin', 'Number of businesses', \n",
    "               'Downtown / Union Square', 'Civic Center', 'Reports 365 days ago',\n",
    "               'Reports 1 day ago','Reports 2 days ago','Reports 14 days ago'],\n",
    "    ADABOOST_NAME : ['Reports 365 days ago', 'Reports 1 day ago', 'Reports 14 days ago', 'Reports 3 days ago', \n",
    "               'Reports 2 days ago', 'Reports 7 days ago', 'Number of businesses',\n",
    "               'Reports 4 days ago','Reports 5 days ago','Closures 365 days ago'],\n",
    "    EQUAL_DATA_NAME : ['Number of businesses', 'Last 28 days closures', 'Last 7 days openings',\n",
    "                          'Last 14 days closures', 'Last 7 days closures','Reports 1 day ago',\n",
    "                      'Reports 2 days ago', 'Reports 4 days ago', 'Reports 30 days ago', 'Reports 7 days ago'],\n",
    "    ALL_BUS_NAME : ['Number of businesses', 'Last 28 days closures', 'Last 7 days openings',\n",
    "                          'Last 14 days closures', 'Last 7 days closures','Number of openings',\n",
    "                   'Openings 4 days ago','Openings 1 day ago', 'Openings 7 days ago', 'Openings 2 days ago']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_hotspots = 5\n",
    "num_hotspots_range = [10]\n",
    "top_x = 5\n",
    "expected_cols_in_bars = 5\n",
    "expected_num_bars = 6\n",
    "expected_scores = [1,1,1,1,1,0]\n",
    "expected_x_ticks = ['Multi\\nLayer\\nPerceptron\\nusing\\nAdaboost\\ndataset',\n",
    "                    'Multi\\nLayer\\nPerceptron\\nusing\\nAdaboost\\ndataset',\n",
    "                    'Multi\\nLayer\\nPerceptron\\nusing\\nAdaboost\\ndataset',\n",
    "                    'Multi\\nLayer\\nPerceptron\\nusing\\nAdaboost\\ndataset',\n",
    "                    'Multi\\nLayer\\nPerceptron\\nusing\\nAdaboost\\ndataset',\n",
    "                    'Multi\\nLayer\\nPerceptron\\nusing\\nF Regression\\ndataset']\n",
    "expected_titles = ['Top 5 Sensitivity scores\\nwhen predicting the top 10 Crime Hotspots',\n",
    "                  'Top 5 Specificity scores\\nwhen predicting the top 10 Crime Hotspots',\n",
    "                  'Top 5 Precision scores\\nwhen predicting the top 10 Crime Hotspots',\n",
    "                  'Top 5 F1 scores\\nwhen predicting the top 10 Crime Hotspots',\n",
    "                  'Top 5 MCC scores\\nwhen predicting the top 10 Crime Hotspots',\n",
    "                  'Top 5 Lowest Misclassification Severity scores\\nwhen predicting the top 10 Crime Hotspots']\n",
    "expected_x_label = \"Algorithm\"\n",
    "expected_y_labels = ['Sensitivity score', 'Specificity score', 'Precision score', 'F1 score',\n",
    "                    'MCC score', 'Lowest Misclassification Severity score']\n",
    "test_evaluate_models(num_hotspots_range, top_x, file_name, expected_cols_in_bars, expected_scores,expected_num_bars,\n",
    "                    expected_x_ticks,expected_titles,expected_x_label, expected_y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test evaluate models in depth <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluate_models_in_depth(num_hotspots_range, top_x, file_name, expected_scores, expected_names):\n",
    "    df = open_file(file_name)\n",
    "    scores, names = evaluate_models_in_depth(num_hotspots_range, top_x, df)\n",
    "    assert scores == expected_scores, \"Scores not as expected.\"\n",
    "    for key in scores:\n",
    "        assert key in expected_scores, \"Scores not as expected.\"\n",
    "        assert scores[key] == expected_scores[key], \"Scores not as expected.\"\n",
    "    assert names == expected_names, \"Names not as expected.\"\n",
    "    for key in names:\n",
    "        assert key in expected_names, \"Names not as expected.\"\n",
    "        assert names[key] == expected_names[key], \"Names not as expected.\"\n",
    "    print(\"All tests completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"tuning_test_data\"\n",
    "model_key = \"tuning_template_test_model\"\n",
    "feature_key = \"arbitrary_name\"\n",
    "MODEL_FILE_TAGS[model_key] = model_key\n",
    "FEATURE_FILE_TAGS[feature_key] = feature_key\n",
    "MODELS = {model_key : model_key}\n",
    "FEATURES = {feature_key : ['Reports 1 day ago', 'Reports 2 days ago', 'Reports 3 days ago',\n",
    "                       'Reports 4 days ago', 'Reports 5 days ago', 'Reports 6 days ago',\n",
    "                      'Reports 7 days ago','Reports 14 days ago','Reports 30 days ago','Reports 365 days ago']}\n",
    "num_hotspots = 5\n",
    "num_hotspots_range = [5]\n",
    "top_x = 5\n",
    "expected_scores = {'5': {'Sensitivity': [1.0], \n",
    "                         'Specificity': [1.0], \n",
    "                         'Precision': [1.0], \n",
    "                         'F1': [1.0], \n",
    "                         'MCC': [1.0], \n",
    "                         'Lowest Misclassification Severity': [0.0]}}\n",
    "expected_names = {'5': {'Sensitivity': ['Multi Layer Perceptron using F Regression dataset'], \n",
    "                        'Specificity': ['Multi Layer Perceptron using F Regression dataset'], \n",
    "                        'Precision': ['Multi Layer Perceptron using F Regression dataset'], \n",
    "                        'F1': ['Multi Layer Perceptron using F Regression dataset'], \n",
    "                        'MCC': ['Multi Layer Perceptron using F Regression dataset'], \n",
    "                        'Lowest Misclassification Severity': ['Multi Layer Perceptron using F Regression dataset']}}\n",
    "test_evaluate_models_in_depth(num_hotspots_range, top_x, file_name, expected_scores, expected_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test best x scores no graph <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best_x_scores_no_graph(score_metric, x, algorithms, results, num_hotspots, expected_scores, expected_names):\n",
    "    scores, names = best_x_scores_no_graph(score_metric, x, algorithms, results, num_hotspots)\n",
    "    assert scores == expected_scores, \"Scores not as expected.\"\n",
    "    for i in range(0,len(scores)):\n",
    "        assert scores[i] == expected_scores[i], \"Scores not as expected.\"\n",
    "    assert names == expected_names, \"Names not as expected.\"\n",
    "    for i in range(len(names)):\n",
    "        assert names[i] == expected_names[i], \"Names not as expected.\"\n",
    "    print(\"All tests completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "algorithms, algorithm_display_names, feature_select_display_names, highest_is_best, score_metrics = setup_lists()\n",
    "scores = {'Sensitivity': [1,2],\n",
    "         'Specificity': [1],\n",
    "         'Precision':[1],\n",
    "         'F1':[1],\n",
    "         'MCC':[1],\n",
    "         'Lowest Misclassification Severity':[2,1,3,7,8,5,4]}\n",
    "metric = 'Lowest Misclassification Severity'\n",
    "x = 5\n",
    "num_hotspots = 5\n",
    "expected_best_scores = [1, 2, 3, 4, 5]\n",
    "expected_names = ['Multi Layer Perceptron using Chi2 dataset', \n",
    "                  'Multi Layer Perceptron using F Regression dataset', \n",
    "                  'Multi Layer Perceptron using Adaboost dataset', \n",
    "                  'Decision Tree using Chi2 dataset', \n",
    "                  'Decision Tree using F Regression dataset']\n",
    "test_best_x_scores_no_graph(metric, x, algorithms, scores, num_hotspots, expected_best_scores, expected_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test get full analysis <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_full_analysis(results, names, num_hotspots_range, expected_num_hotspots, expected_best, expected_r1,\n",
    "                           expected_r2, expected_r3, expected_r4, expected_r5):\n",
    "    table = get_full_analysis(results, names, num_hotspots_range)\n",
    "    columns = table.columns\n",
    "    for col in columns:\n",
    "        assert col in expected_columns, \"Table columns not as expected.\"\n",
    "    for col in expected_columns:\n",
    "        assert col in columns, \"Table columns not as expected.\"\n",
    "    num_hotspots = table['Number of Hotspots Predicted'].tolist()\n",
    "    for i in range (0,len(num_hotspots)):\n",
    "        assert num_hotspots[i] == expected_num_hotspots[i], \"Number of hotspots column not as expected.\"\n",
    "    best = table['Best Score'].tolist()\n",
    "    for i in range (0,len(best)):\n",
    "        assert best[i] == expected_best[i], \"Best score column not as expected.\"\n",
    "    r1 = table['Rank 1'].tolist()\n",
    "    for i in range (0,len(r1)):\n",
    "        assert r1[i] == expected_r1[i], \"Rank 1 column not as expected.\"\n",
    "    r2 = table['Rank 2'].tolist()\n",
    "    for i in range (0,len(r2)):\n",
    "        assert r2[i] == expected_r2[i], \"Rank 2 column not as expected.\"\n",
    "    r3 = table['Rank 3'].tolist()\n",
    "    for i in range (0,len(r3)):\n",
    "        assert r3[i] == expected_r3[i], \"Rank 3 column  not as expected.\"\n",
    "    r4 = table['Rank 4'].tolist()\n",
    "    for i in range (0,len(r4)):\n",
    "        assert r4[i] == expected_r4[i], \"Rank 4 column  not as expected.\"\n",
    "    r5 = table['Rank 5'].tolist()\n",
    "    for i in range (0,len(r5)):\n",
    "        assert r5[i] == expected_r5[i], \"Rank 5 column  not as expected.\"\n",
    "    print(\"All tests completed successfully\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'1': {'Sensitivity': [10,9,8,7,6,5,4,3,2,1],\n",
    "                 'Specificity': [10,9,8,7,6,5,4,3,2,1],\n",
    "                 'Precision': [10,9,8,7,6,5,4,3,2,1],\n",
    "                 'F1': [10,9,8,7,6,5,4,3,2,1],\n",
    "                 'MCC': [10,9,8,7,6,5,4,3,2,1],\n",
    "                 'Lowest Misclassification Severity': [1,2,3,4,5,6,7,8,9,10]},\n",
    "           '2': {'Sensitivity': [110,19,18,17,16,15,14,13,12,11],\n",
    "                 'Specificity': [110,19,18,17,16,15,14,13,12,11],\n",
    "                 'Precision': [110,19,18,17,16,15,14,13,12,11],\n",
    "                 'F1': [110,19,18,17,16,15,14,13,12,11],\n",
    "                 'MCC': [110,19,18,17,16,15,14,13,12,11],\n",
    "                 'Lowest Misclassification Severity': [11,12,13,14,15,16,17,18,19,110]},\n",
    "          '3': {'Sensitivity': [210,29,28,27,26,25,24,23,22,21],\n",
    "                 'Specificity': [210,29,28,27,26,25,24,23,22,21],\n",
    "                 'Precision': [210,29,28,27,26,25,24,23,22,21],\n",
    "                 'F1': [210,29,28,27,26,25,24,23,22,21],\n",
    "                 'MCC': [210,29,28,27,26,25,24,23,22,21],\n",
    "                 'Lowest Misclassification Severity': [21,22,23,24,25,26,27,28,29,210]}}\n",
    "names = {'1': {'Sensitivity': [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\"],\n",
    "                 'Specificity': [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\"],\n",
    "                 'Precision': [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\"],\n",
    "                 'F1': [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\"],\n",
    "                 'MCC': [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\"],\n",
    "                 'Lowest Misclassification Severity': [\"A10\",\"A9\",\"A8\",\"A7\",\"A6\",\"A5\",\"A4\",\"A3\",\"A2\",\"A1\"]},\n",
    "         '2': {'Sensitivity': [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\"],\n",
    "                 'Specificity': [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\"],\n",
    "                 'Precision': [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\"],\n",
    "                 'F1': [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\"],\n",
    "                 'MCC': [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\"],\n",
    "                 'Lowest Misclassification Severity': [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\"]},\n",
    "         '3': {'Sensitivity': [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\"],\n",
    "                 'Specificity': [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\"],\n",
    "                 'Precision': [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\"],\n",
    "                 'F1': [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\"],\n",
    "                 'MCC': [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\"],\n",
    "                 'Lowest Misclassification Severity': [\"A10\",\"A9\",\"A8\",\"A7\",\"A6\",\"A5\",\"A4\",\"A3\",\"A2\",\"A1\"]}}\n",
    "num_hotspots_range = [1,2]\n",
    "expected_columns = ['Number of Hotspots Predicted','Best Score','Rank 1','Rank 2','Rank 3','Rank 4','Rank 5']\n",
    "expected_num_hotspots = [1,2]\n",
    "expected_best = [1,11]\n",
    "expected_r1 = ['A10','A1']\n",
    "expected_r2 = ['A9','A2']\n",
    "expected_r3 = ['A8','A3']\n",
    "expected_r4 = ['A7','A4']\n",
    "expected_r5 = ['A6','A5']\n",
    "test_get_full_analysis(results, names, num_hotspots_range, expected_num_hotspots, expected_best, expected_r1,\n",
    "                       expected_r2, expected_r3, expected_r4, expected_r5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test get score by rank <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_score_by_rank(scores, ranks, expected_scores):\n",
    "    for i in range(0, len(ranks)):\n",
    "        score = get_score_by_rank(scores, ranks[i])\n",
    "        assert score == expected_scores[i], \"Scores not as expected. i:\" + str(i) + \"Score:\" + str(score) + \"Exp Score:\" + str(expected_scores[i])\n",
    "    print(\"All tests completed successfully\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [1,1,1,2,3,3,4,4,4,4,4,5,5,5,5,5,6,7,8,9,10]\n",
    "ranks = [1,2,3,4,5,6,7,8,9,10,11]\n",
    "expected_scores = [1,2,3,4,5,6,7,8,9,10,0]\n",
    "test_get_score_by_rank(scores, ranks, expected_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test Get Average Score Per Num Hotspots <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_average_score_per_num_hotspots(algorithms, results, names, num_hotspots_range,expected_averages):\n",
    "    averages = get_average_score_per_num_hotspots(algorithms, results, names, num_hotspots_range)\n",
    "    assert averages == expected_averages, \"Averages not as expected\"\n",
    "    for i in range (0, len(expected_averages)):\n",
    "        assert averages[i] == expected_averages[i], \"Averages not as expected\"\n",
    "    print(\"All tests completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['A1','A2','A3','A4','A5','A6','A7','A8','A9','A10']\n",
    "results = {'1': {'Lowest Misclassification Severity': [1,2,3,4,5,6,7,8,9,10]},\n",
    "           '2': {'Lowest Misclassification Severity': [11,12,13,14,15,16,17,18,19,20]},\n",
    "          '3': {'Lowest Misclassification Severity': [21,22,23,24,25,26,27,28,29,210]}}\n",
    "names = {'1': {'Lowest Misclassification Severity': [\"A10\",\"A9\",\"A8\",\"A7\",\"A6\",\"A5\",\"A4\",\"A3\",\"A2\",\"A1\"]},\n",
    "         '2': {'Lowest Misclassification Severity': [\"A10\",\"A9\",\"A8\",\"A7\",\"A6\",\"A5\",\"A4\",\"A3\",\"A2\",\"A1\"]},\n",
    "         '3': {'Lowest Misclassification Severity': [\"A10\",\"A9\",\"A8\",\"A7\",\"A6\",\"A5\",\"A4\",\"A3\",\"A2\",\"A1\"]}}\n",
    "num_hotspots_range = [1, 2]\n",
    "expected_averages = [15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0]\n",
    "test_get_average_score_per_num_hotspots(algorithms, results, names, num_hotspots_range,expected_averages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test get best counts <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_best_counts(results, names, num_hotspots_range, rank, expected_algorithms, expected_averages, expected_times_ranked,\n",
    "                    expected_hotspots, expected_columns):\n",
    "    table = get_best_counts(results, names, num_hotspots_range, rank)\n",
    "    columns = table.columns\n",
    "    for col in columns:\n",
    "        assert col in expected_columns, \"Table columns not as expected.\"\n",
    "    for col in expected_columns:\n",
    "        assert col in columns, \"Table columns not as expected.\"\n",
    "    algorithms = table['Algorithm'].tolist()\n",
    "    for i in range (0,len(algorithms)):\n",
    "        assert algorithms[i] == expected_algorithms[i], \"Algorithm column not as expected.\"\n",
    "    \n",
    "    averages = table['Average Score'].tolist()\n",
    "    for i in range (0,len(averages)):\n",
    "        assert averages[i] == expected_averages[i], \"Average Score column not as expected.\"\n",
    "    \n",
    "    times_ranked = table['Times Ranked 1'].tolist()\n",
    "    for i in range (0,len(times_ranked)):\n",
    "        assert times_ranked[i] == expected_times_ranked[i], \"Times Ranked column not as expected.\"\n",
    "    \n",
    "    hotspots = table['Hotspot Values'].tolist()\n",
    "    for i in range (0,len(hotspots)):\n",
    "        assert hotspots[i] == expected_hotspots[i], \"Hotspots column not as expected.\"\n",
    "    print(\"All tests passed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'1': {'Lowest Misclassification Severity': [1,2,3,4,5,6,7,8,9,10]},\n",
    "           '2': {'Lowest Misclassification Severity': [2,3,4,5,6,7,8,9,10,11]},\n",
    "          '3': {'Lowest Misclassification Severity': [3,4,5,6,7,8,9,10,11,12]},\n",
    "          '4': {'Lowest Misclassification Severity': [4,5,6,7,8,9,10,11,12,13]},\n",
    "          '5': {'Lowest Misclassification Severity': [5,6,7,8,9,10,11,12,13,14]},\n",
    "          '6': {'Lowest Misclassification Severity': [6,7,8,9,10,11,12,13,14,15]},\n",
    "          '7': {'Lowest Misclassification Severity': [7,8,9,10,11,12,13,14,15,16]},\n",
    "          '8': {'Lowest Misclassification Severity': [8,9,10,11,12,13,14,15,16,17]},\n",
    "          '9': {'Lowest Misclassification Severity': [9,10,11,12,13,14,15,16,17,18]},\n",
    "          '10': {'Lowest Misclassification Severity': [10,11,12,13,14,15,16,17,18,19]}}\n",
    "names = {'1': {'Lowest Misclassification Severity': [\"A10\",\"A9\",\"A8\",\"A7\",\"A6\",\"A5\",\"A4\",\"A3\",\"A2\",\"A1\"]},\n",
    "         '2': {'Lowest Misclassification Severity': [\"A10\",\"A9\",\"A8\",\"A7\",\"A6\",\"A5\",\"A4\",\"A3\",\"A2\",\"A1\"]},\n",
    "         '3': {'Lowest Misclassification Severity': [\"A10\",\"A9\",\"A8\",\"A7\",\"A6\",\"A5\",\"A4\",\"A3\",\"A2\",\"A1\"]},\n",
    "        '4': {'Lowest Misclassification Severity': [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\"]},\n",
    "        '5': {'Lowest Misclassification Severity': [\"A5\",\"A4\",\"A3\",\"A2\",\"A1\",\"A7\",\"A6\",\"A8\",\"A9\",\"A10\"]},\n",
    "        '6': {'Lowest Misclassification Severity': [\"A1\",\"A10\",\"A9\",\"A8\",\"A7\",\"A6\",\"A5\",\"A4\",\"A3\",\"A2\"]},\n",
    "        '7': {'Lowest Misclassification Severity': [\"A6\",\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A10\",\"A7\",\"A8\",\"A9\"]},\n",
    "        '8': {'Lowest Misclassification Severity': [\"A5\",\"A1\",\"A2\",\"A3\",\"A4\",\"A10\",\"A6\",\"A7\",\"A8\",\"A9\"]},\n",
    "        '9': {'Lowest Misclassification Severity': [\"A1\",\"A5\",\"A4\",\"A3\",\"A2\",\"A6\",\"A7\",\"A9\",\"A8\",\"A10\"]},\n",
    "        '10': {'Lowest Misclassification Severity': [\"A10\",\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\"]}}\n",
    "num_hotspots_range = [1, 2, 3,4,5,6,7,8,9,10]\n",
    "rank = 1\n",
    "expected_columns = [\"Algorithm\", \"Average Score\", \"Times Ranked 1\", \"Hotspot Values\"]\n",
    "expected_algorithms = ['A10','A1','A5','A6']\n",
    "expected_averages = [9.4,8.9,9.1,10.0]\n",
    "expected_times_ranked = [4,3,2,1]\n",
    "expected_hotspots = [\"1 2 3 10 \", \"4 6 9 \", \"5 8 \", \"7 \"]\n",
    "test_get_best_counts(results, names, num_hotspots_range, rank, expected_algorithms, expected_averages, expected_times_ranked,\n",
    "                    expected_hotspots, expected_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
